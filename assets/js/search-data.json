{
  
    
        "post0": {
            "title": "Open positions",
            "content": "We seek highly motivated candidates to work on the development of Brain Computer Interfaces (BCI) and assistive robotic systems to restore human motor functions after stroke, as well as devising machine learning algorithms for BCIs. This research is multidisciplinary, and we collaborate with other laboratories in South Korea, the USA, and medical centers in Nur-Sultan, thus providing great educational and networking opportunities. . Desired qualifications: Experience in signal processing, pattern recognition/machine learning; experience in neural engineering (e.g., Brain-Computer Interfaces, neuroprosthetics). Candidates should possess proficiency with Matlab/Python or C/C++. . Contact berdakh.abibullaev at nu.edu.kz for further details with your CV. .",
            "url": "https://berdakh.github.io/blog/markdown/2020/12/05/Opening.html",
            "relUrl": "/markdown/2020/12/05/Opening.html",
            "date": " • Dec 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Motor-Imagery BCI decoding",
            "content": "Theory:: Introduction to Motor Imagery BCIs . from IPython.display import YouTubeVideo YouTubeVideo(&#39;ch0K0iFaUX8&#39;, width=800, height=400) . Practice:: Introduction to Motor Imagery BCIs . from IPython.display import YouTubeVideo YouTubeVideo(&#39;EAQcu6DLAS0&#39;, width=800, height=400) . Credits . By Marijn van Vliet, 2017. The CSP code was originally written by Boris Reuderink and Marijn van Vliet of the Donders Institute for Brain, Cognition and Behavior. It is part of his Python EEG toolbox: https://github.com/breuderink/eegtools Adopted from Marijn van Vliet, 2017. | . | . Obtaining the data . The dataset for this tutorial is provided by the fourth BCI competition, which you will have to download youself. First, go to http://www.bbci.de/competition/iv/#download and fill in your name and email address. An email will be sent to you automatically containing a username and password for the download area. . Download Data Set 1, from Berlin, the 100Hz version in MATLAB format: http://bbci.de/competition/download/competition_iv/BCICIV_1_mat.zip and unzip it in a subdirectory called &#39;data_set_IV&#39;. This subdirectory should be inside the directory in which you&#39;ve store the tutorial files. . Description of the data . If you&#39;ve followed the instructions above, the following code should load the data: . import numpy as np import scipy.io # load the matlab m = scipy.io.loadmat(&#39;BCICIV_calib_ds1d.mat&#39;, struct_as_record=True) . sample_rate = m[&#39;nfo&#39;][&#39;fs&#39;][0][0][0][0] EEG = m[&#39;cnt&#39;].T nchannels, nsamples = EEG.shape . channel_names = [s[0] for s in m[&#39;nfo&#39;][&#39;clab&#39;][0][0][0]] event_onsets = m[&#39;mrk&#39;][0][0][0] event_codes = m[&#39;mrk&#39;][0][0][1] . labels = np.zeros((1, nsamples), int) labels[0, event_onsets] = event_codes cl_lab = [s[0] for s in m[&#39;nfo&#39;][&#39;classes&#39;][0][0][0]] cl1 = cl_lab[0] cl2 = cl_lab[1] nclasses = len(cl_lab) nevents = len(event_onsets) . Now we have the data in the following python variables: Print some information . # Dictionary to store the trials in, each class gets an entry trials = {} # The time window (in samples) to extract for each trial, here 0.5 -- 2.5 seconds win = np.arange(int(0.5*sample_rate), int(2.5*sample_rate)) # Length of the time window nsamples = len(win) . # Loop over the classes (right, foot) for cl, code in zip(cl_lab, np.unique(event_codes)): # Extract the onsets for the class cl_onsets = event_onsets[event_codes == code] # Allocate memory for the trials trials[cl] = np.zeros((nchannels, nsamples, len(cl_onsets))) # Extract each trial for i, onset in enumerate(cl_onsets): trials[cl][:,:,i] = EEG[:, win+onset] # Some information about the dimensionality of the data (channels x time x trials) print(&#39;Shape of trials[cl1]:&#39;, trials[cl1].shape) print(&#39;Shape of trials[cl2]:&#39;, trials[cl2].shape) . Shape of trials[cl1]: (59, 200, 100) Shape of trials[cl2]: (59, 200, 100) . Since the feature we&#39;re looking for (a decrease in $ mu$-activity) is a frequency feature, lets plot the PSD of the trials. The code below defines a function that computes the PSD for each trial (we&#39;re going to need it again later on): . from matplotlib import mlab def psd(trials): &#39;&#39;&#39; Calculates for each trial the Power Spectral Density (PSD). Parameters - trials : 3d-array (channels x samples x trials) The EEG signal Returns - trial_PSD : 3d-array (channels x PSD x trials) the PSD for each trial. freqs : list of floats Yhe frequencies for which the PSD was computed (useful for plotting later) &#39;&#39;&#39; ntrials = trials.shape[2] trials_PSD = np.zeros((nchannels, 101, ntrials)) # Iterate over trials and channels for trial in range(ntrials): for ch in range(nchannels): # Calculate the PSD (PSD, freqs) = mlab.psd(trials[ch,:,trial], NFFT=int(nsamples), Fs=sample_rate) trials_PSD[ch, :, trial] = PSD.ravel() return trials_PSD, freqs . # Apply the function psd_r, freqs = psd(trials[cl1]) psd_f, freqs = psd(trials[cl2]) trials_PSD = {cl1: psd_r, cl2: psd_f} . import matplotlib.pyplot as plt def plot_psd(trials_PSD, freqs, chan_ind, chan_lab=None, maxy=None): &#39;&#39;&#39; Plots PSD data calculated with psd(). Parameters - trials : 3d-array The PSD data, as returned by psd() freqs : list of floats The frequencies for which the PSD is defined, as returned by psd() chan_ind : list of integers The indices of the channels to plot chan_lab : list of strings (optional) List of names for each channel maxy : float (optional) Limit the y-axis to this value &#39;&#39;&#39; plt.figure(figsize=(12,5)) nchans = len(chan_ind) # Maximum of 3 plots per row nrows = np.ceil(nchans / 3) ncols = min(3, nchans) # Enumerate over the channels for i,ch in enumerate(chan_ind): # Figure out which subplot to draw to plt.subplot(nrows,ncols,i+1) # Plot the PSD for each class for cl in trials.keys(): plt.plot(freqs, np.mean(trials_PSD[cl][ch,:,:], axis=1), label=cl) # All plot decoration below... plt.xlim(1,30) if maxy != None: plt.ylim(0,maxy) plt.grid() plt.xlabel(&#39;Frequency (Hz)&#39;) if chan_lab == None: plt.title(&#39;Channel %d&#39; % (ch+1)) else: plt.title(chan_lab[i]) plt.legend() plt.tight_layout() . # # 1. C3: Central, left # 2. Cz: Central, central # 3. C4: Central, right . plot_psd( trials_PSD, freqs, [channel_names.index(ch) for ch in [&#39;C3&#39;, &#39;Cz&#39;, &#39;C4&#39;]], chan_lab=[&#39;left&#39;, &#39;center&#39;, &#39;right&#39;], maxy=500 ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-03T16:30:14.994098 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ import scipy.signal def bandpass(trials, lo, hi, sample_rate): &#39;&#39;&#39; Designs and applies a bandpass filter to the signal. Parameters - trials : 3d-array (channels x samples x trials) The EEGsignal lo : float Lower frequency bound (in Hz) hi : float Upper frequency bound (in Hz) sample_rate : float Sample rate of the signal (in Hz) Returns - trials_filt : 3d-array (channels x samples x trials) The bandpassed signal &#39;&#39;&#39; # The iirfilter() function takes the filter order: higher numbers mean a sharper frequency cutoff, # but the resulting signal might be shifted in time, lower numbers mean a soft frequency cutoff, # but the resulting signal less distorted in time. It also takes the lower and upper frequency bounds # to pass, divided by the niquist frequency, which is the sample rate divided by 2: a, b = scipy.signal.iirfilter(6, [lo/(sample_rate/2.0), hi/(sample_rate/2.0)]) # Applying the filter to each trial ntrials = trials.shape[2] trials_filt = np.zeros((nchannels, nsamples, ntrials)) for i in range(ntrials): trials_filt[:,:,i] = scipy.signal.filtfilt(a, b, trials[:,:,i], axis=1) return trials_filt . # Apply the function trials_filt = {cl1: bandpass(trials[cl1], 8, 15, sample_rate), cl2: bandpass(trials[cl2], 8, 15, sample_rate)} . Plotting the PSD of the resulting trials_filt shows the suppression of frequencies outside the passband of the filter: . psd_r, freqs = psd(trials_filt[cl1]) psd_f, freqs = psd(trials_filt[cl2]) trials_PSD = {cl1: psd_r, cl2: psd_f} plot_psd( trials_PSD, freqs, [channel_names.index(ch) for ch in [&#39;C3&#39;, &#39;Cz&#39;, &#39;C4&#39;]], chan_lab=[&#39;left&#39;, &#39;center&#39;, &#39;right&#39;], maxy=300 ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-03T16:30:17.057808 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ As a feature for the classifier, we will use the logarithm of the variance of each channel. The function below calculates this: . # Calculate the log(var) of the trials def logvar(trials): &#39;&#39;&#39; Calculate the log-var of each channel. Parameters - trials : 3d-array (channels x samples x trials) The EEG signal. Returns - logvar - 2d-array (channels x trials) For each channel the logvar of the signal &#39;&#39;&#39; return np.log(np.var(trials, axis=1)) . # Apply the function trials_logvar = {cl1: logvar(trials_filt[cl1]), cl2: logvar(trials_filt[cl2])} . Below is a function to visualize the logvar of each channel as a bar chart: . def plot_logvar(trials): &#39;&#39;&#39; Plots the log-var of each channel/component. arguments: trials - Dictionary containing the trials (log-vars x trials) for 2 classes. &#39;&#39;&#39; plt.figure(figsize=(12,5)) x0 = np.arange(nchannels) x1 = np.arange(nchannels) + 0.4 y0 = np.mean(trials[cl1], axis=1) y1 = np.mean(trials[cl2], axis=1) plt.bar(x0, y0, width=0.5, color=&#39;b&#39;) plt.bar(x1, y1, width=0.4, color=&#39;r&#39;) plt.xlim(-0.5, nchannels+0.5) plt.gca().yaxis.grid(True) plt.title(&#39;log-var of each channel/component&#39;) plt.xlabel(&#39;channels/components&#39;) plt.ylabel(&#39;log-var&#39;) plt.legend(cl_lab) . # Plot the log-vars plot_logvar(trials_logvar) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-03T16:30:17.931738 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ from numpy import linalg def cov(trials): &#39;&#39;&#39; Calculate the covariance for each trial and return their average &#39;&#39;&#39; ntrials = trials.shape[2] covs = [ trials[:,:,i].dot(trials[:,:,i].T) / nsamples for i in range(ntrials) ] return np.mean(covs, axis=0) def whitening(sigma): &#39;&#39;&#39; Calculate a whitening matrix for covariance matrix sigma. &#39;&#39;&#39; U, l, _ = linalg.svd(sigma) return U.dot(np.diag(l ** -0.5) ) def csp(trials_r, trials_f): &#39;&#39;&#39; Calculate the CSP transformation matrix W. arguments: trials_r - Array (channels x samples x trials) containing right hand movement trials trials_f - Array (channels x samples x trials) containing foot movement trials returns: Mixing matrix W &#39;&#39;&#39; cov_r = cov(trials_r) cov_f = cov(trials_f) P = whitening(cov_r + cov_f) B, _, _ = linalg.svd( P.T.dot(cov_f).dot(P) ) W = P.dot(B) return W def apply_mix(W, trials): &#39;&#39;&#39; Apply a mixing matrix to each trial (basically multiply W with the EEG signal matrix)&#39;&#39;&#39; ntrials = trials.shape[2] trials_csp = np.zeros((nchannels, nsamples, ntrials)) for i in range(ntrials): trials_csp[:,:,i] = W.T.dot(trials[:,:,i]) return trials_csp . # Apply the functions W = csp(trials_filt[cl1], trials_filt[cl2]) W . array([[ 8.91637823e-03, 4.09326627e-02, -1.93570109e-02, ..., -3.58699325e-02, 5.77671413e-02, -2.82445586e-03], [-3.16087953e-03, -8.46988629e-03, -1.84693547e-02, ..., 6.42172317e-03, -2.40078430e-04, -3.38970192e-03], [ 8.68684914e-04, -4.03607636e-02, 5.85203824e-02, ..., -5.26239473e-05, -9.22199205e-02, 8.82055215e-02], ..., [ 1.20184885e-02, -3.07688850e-02, 9.92981869e-03, ..., 4.35671271e-03, 2.32770809e-02, -1.16083121e-02], [-1.50527952e-03, -1.90034370e-02, -8.64302846e-03, ..., 2.61831782e-02, -1.53422660e-02, -5.86193003e-04], [-2.39667163e-03, 3.71515052e-03, -7.49688719e-03, ..., 1.23402088e-02, 1.68134070e-02, -1.06752302e-04]]) . trials_csp = {cl1: apply_mix(W, trials_filt[cl1]), cl2: apply_mix(W, trials_filt[cl2])} . To see the result of the CSP algorithm, we plot the log-var like we did before: . trials_logvar = {cl1: logvar(trials_csp[cl1]), cl2: logvar(trials_csp[cl2])} plot_logvar(trials_logvar) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-03T16:30:18.688036 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ psd_r, freqs = psd(trials_csp[cl1]) psd_f, freqs = psd(trials_csp[cl2]) trials_PSD = {cl1: psd_r, cl2: psd_f} plot_psd(trials_PSD, freqs, [0,58,-1], chan_lab=[&#39;first component&#39;, &#39;middle component&#39;, &#39;last component&#39;], maxy=0.75 ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-03T16:30:20.387509 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ def plot_scatter(left, foot): plt.figure() plt.scatter(left[0,:], left[-1,:], color=&#39;b&#39;) plt.scatter(foot[0,:], foot[-1,:], color=&#39;r&#39;) plt.xlabel(&#39;Last component&#39;) plt.ylabel(&#39;First component&#39;) plt.legend(cl_lab) . plot_scatter(trials_logvar[cl1], trials_logvar[cl2]) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-03T16:30:20.957084 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Training a classifier . # Percentage of trials to use for training (50-50 split here) train_percentage = 0.5 # Calculate the number of trials for each class the above percentage boils down to ntrain_r = int(trials_filt[cl1].shape[2] * train_percentage) ntrain_f = int(trials_filt[cl2].shape[2] * train_percentage) . ntest_r = trials_filt[cl1].shape[2] - ntrain_r ntest_f = trials_filt[cl2].shape[2] - ntrain_f # Splitting the frequency filtered signal into a train and test set train = {cl1: trials_filt[cl1][:,:,:ntrain_r], cl2: trials_filt[cl2][:,:,:ntrain_f]} test = {cl1: trials_filt[cl1][:,:,ntrain_r:], cl2: trials_filt[cl2][:,:,ntrain_f:]} . Train the CSP on the training set only . W = csp(train[cl1], train[cl2]) . train[cl1].shape . (59, 200, 50) . W.shape . (59, 59) . Apply the CSP on both the training and test set . train[cl1] = apply_mix(W, train[cl1]) train[cl2] = apply_mix(W, train[cl2]) test[cl1] = apply_mix(W, test[cl1]) test[cl2] = apply_mix(W, test[cl2]) . train[cl1].shape . (59, 200, 50) . Select only the first and last components for classification . comp = np.array([0,-1]) train[cl1] = train[cl1][comp,:,:] train[cl2] = train[cl2][comp,:,:] test[cl1] = test[cl1][comp,:,:] test[cl2] = test[cl2][comp,:,:] . train[cl1].shape . (2, 200, 50) . Calculate the log-var . train[cl1] = logvar(train[cl1]) train[cl2] = logvar(train[cl2]) test[cl1] = logvar(test[cl1]) test[cl2] = logvar(test[cl2]) . train[cl1].shape . (2, 50) . train[cl1].T . array([[-1.78873435, -0.42012736], [-1.25826247, -0.24579711], [-1.62208368, 0.07604777], [-1.13462492, -0.63162286], [-0.85817442, -0.27040373], [-0.51468751, -0.80345916], [-0.68589754, 0.31010884], [-1.24083025, -0.54478613], [-1.3031056 , -0.41542025], [-1.86494961, 0.68382534], [-1.66420451, -0.80799868], [-1.72840572, -0.29365595], [-1.1900163 , -0.02820363], [-1.84248844, -0.24632442], [-1.63062094, -1.0089116 ], [-1.59233353, -1.04830558], [-2.32193295, -0.40343997], [-0.56121766, -0.93885695], [-1.71918374, 0.31301148], [-1.67358494, -0.39890108], [-0.91830957, -1.61477425], [-1.25688826, -0.4421753 ], [-1.45622913, 0.02785208], [-0.97478482, -0.2044433 ], [-1.19566739, -0.7279463 ], [-1.24090092, 0.10491371], [-1.08492422, -0.08910298], [-1.15749597, -0.27843615], [-0.5966315 , -0.49519372], [-1.41304097, -0.37371781], [-0.50827517, -0.96201348], [-1.44776168, -0.35228259], [-1.53280914, -0.71149566], [-1.57790959, -0.07606625], [-1.07766873, -0.29456733], [-1.82515131, -1.19028611], [-1.25723906, -0.31170181], [-1.21507642, 0.30639431], [-1.34860386, -0.17379919], [-1.59298012, -1.16031317], [-1.63405007, -0.51888977], [-1.44366868, -0.49968391], [-1.5374963 , -0.667521 ], [-1.89810484, -0.6930739 ], [-1.30772478, 0.23814583], [-1.47074803, -0.25386212], [-1.27730317, -0.52634775], [-1.64335898, 0.36783792], [-1.15077149, -1.33493563], [-1.42430397, -0.02165483]]) . def train_lda(class1, class2): &#39;&#39;&#39; Trains the LDA algorithm. arguments: class1 - An array (observations x features) for class 1 class2 - An array (observations x features) for class 2 returns: The projection matrix W The offset b &#39;&#39;&#39; nclasses = 2 nclass1 = class1.shape[0] nclass2 = class2.shape[0] # Class priors: in this case, we have an equal number of training # examples for each class, so both priors are 0.5 prior1 = nclass1 / float(nclass1 + nclass2) prior2 = nclass2 / float(nclass1 + nclass1) mean1 = np.mean(class1, axis=0) mean2 = np.mean(class2, axis=0) class1_centered = class1 - mean1 class2_centered = class2 - mean2 # Calculate the covariance between the features cov1 = class1_centered.T.dot(class1_centered) / (nclass1 - nclasses) cov2 = class2_centered.T.dot(class2_centered) / (nclass2 - nclasses) W = (mean2 - mean1).dot(np.linalg.pinv(prior1*cov1 + prior2*cov2)) b = (prior1*mean1 + prior2*mean2).dot(W) return (W,b) def apply_lda(test, W, b): &#39;&#39;&#39; Applies a previously trained LDA to new data. arguments: test - An array (features x trials) containing the data W - The project matrix W as calculated by train_lda() b - The offsets b as calculated by train_lda() returns: A list containing a classlabel for each trial &#39;&#39;&#39; ntrials = test.shape[1] prediction = [] for i in range(ntrials): # The line below is a generalization for: # result = W[0] * test[0,i] + W[1] * test[1,i] - b result = W.dot(test[:,i]) - b if result &lt;= 0: prediction.append(1) else: prediction.append(2) return np.array(prediction) . Training the LDA using the training data gives us $W$ and $b$: . W,b = train_lda(train[cl1].T, train[cl2].T) print(&#39;W:&#39;, W) print(&#39;b:&#39;, b) . W: [ 5.31347949 -5.52963938] b: 0.38024720994749295 . We first plot the decision boundary with the training data used to calculate it: . # Scatterplot like before plot_scatter(train[cl1], train[cl2]) # Calculate decision boundary (x,y) x = np.arange(-5, 1, 0.1) y = (b - W[0]*x) / W[1] # Plot the decision boundary plt.plot(x,y, linestyle=&#39;--&#39;, linewidth=2, color=&#39;k&#39;) plt.xlim(-5, 1) plt.ylim(-2.2, 1) . (-2.2, 1.0) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-03T16:30:45.082395 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ The code below plots the boundary with the test data on which we will apply the classifier. You will see the classifier is going to make some mistakes. . plot_scatter(test[cl1], test[cl2]) plt.plot(x,y, linestyle=&#39;--&#39;, linewidth=2, color=&#39;k&#39;) plt.xlim(-5, 1) plt.ylim(-2.2, 1) . (-2.2, 1.0) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-11-03T16:30:54.566172 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ # Print confusion matrix conf = np.array([ [(apply_lda(test[cl1], W, b) == 1).sum(), (apply_lda(test[cl2], W, b) == 1).sum()], [(apply_lda(test[cl1], W, b) == 2).sum(), (apply_lda(test[cl2], W, b) == 2).sum()], ]) print(&#39;Confusion matrix:&#39;) print(conf) print(&#39;Accuracy: %.3f&#39; % (np.sum(np.diag(conf)) / float(np.sum(conf)))) . Confusion matrix: [[45 4] [ 5 46]] Accuracy: 0.910 .",
            "url": "https://berdakh.github.io/blog/jupyter/2020/11/03/motor-imagery-BCIs.html",
            "relUrl": "/jupyter/2020/11/03/motor-imagery-BCIs.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Introduction to Support Vector Machines",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;CG-nUvJgr2Y&#39;, width=800, height=400) . . Step 1. . Define the distance between two points in the space . $$d = ( bf{x_2} - bf{x_1})$$ . Step 2. . Compute the projection of $ bf{d}$ onto $ bf{p}$ vector: . $$proj_{d}p = | bf{x_2} - bf{x_1} |cos( theta)= |x_2 - x_1 | frac{ langle x_2 - x_1, p rangle}{ |x_2 - x_1 | |p |} = frac{ langle x_2 - x_1, p rangle}{ |p |}$$ . where: $$cos( theta) = frac{ langle bf{x_2} - bf{x_1}, p rangle}{ | bf{x_2} - bf{x_1} | |p |}$$ . Step 3. . Represent $ bf{p}$ in terms of $ bf{w}$: . $ bf{p} perp bf{w}$ thus $ bf{p} = lambda bf{w}$, for some $ lambda in R$ and, . $proj_{d} bf{p} = frac{ lambda langle ( bf{x_2} - bf{x_1}), w rangle}{ lambda | bf{w} |}$. . Step 4. . Define the geometrical margin between two hyperplanes for any $x_{i} in mathcal{X}$, . $ big[ langle bf{w}, bf{x_{2}} rangle + b = 1 big] - big[ langle bf{w}, bf{x_{1}} rangle + b = -1 big] = big[ langle bf{w}, bf{x_2}- bf{x_1} rangle = 2 big]$ . $M = proj_{d}p= frac{ langle ( bf{x_2} - bf{x_1}), w rangle}{ | bf{w} |} = frac{2}{ | bf{w} |}$ . Maximize margin . $$ mbox{max}_{ bf{w}, xi} rightarrow frac{2}{ | bf{w} |}+ C sum_{i=1}^{n} xi_{i}$$ . equivalently, we can minimize: . $$ mbox{min}_{ bf{w}, xi} rightarrow frac{1}{2} | bf{w} |+ C sum_{i=1}^{n} xi_{i}$$ . subject to $y_{i} big( langle w cdot x_{i} rangle + b big) geq 1$, $ forall x_{i} in Omega, i = 1,...,n$ . Where: . Set $ Omega = {x_1,x_2,...,x_n }$ $ Omega subset mathcal{X}$ then consider $ {x_1,x_2,...,x_k } in omega_{1}$, $ {x_{k+1},x_{k+2},...,x_{n} } in omega_{2}$ for two classes $ omega_{1}, omega_{2} in Omega$ . Find: $f(x_{i})&gt; 1, forall x_{i} in omega_1$ and $f(x_{i})&lt; -1, forall x_{i} in omega_2$, we can write it as $y_{i} big( langle w cdot x_{i} rangle + b big) geq 1$, where $y = { pm1 }$, and $f(x_i) = y_{i} big( langle w cdot x_{i} rangle + b big)$ .",
            "url": "https://berdakh.github.io/blog/jupyter/2020/10/21/defining-the-SVM-margin.html",
            "relUrl": "/jupyter/2020/10/21/defining-the-SVM-margin.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Supervised learning and model selection with scikit-learn",
            "content": "To visualize the workings of machine learning algorithms, it is often helpful to study two-dimensional or one-dimensional data, that is data with only one or two features. While in practice, datasets usually have many more features, it is hard to plot high-dimensional data in on two-dimensional screens. . We will illustrate some very simple examples before we move on to more &quot;real world&quot; data sets. . First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the make_blobs function. . from IPython.display import YouTubeVideo YouTubeVideo(&#39;wCpCDbkDJXQ&#39;, width=700, height=400) . %matplotlib inline import matplotlib.pyplot as plt import numpy as np . import torch . from sklearn.datasets import make_blobs X, y = make_blobs(centers=2, random_state=0) print(&#39;X ~ n_samples x n_features:&#39;, X.shape) print(&#39;y ~ n_samples:&#39;, y.shape) print(&#39; nFirst 5 samples: n&#39;, X[:5, :]) print(&#39; nFirst 5 labels:&#39;, y[:5]) . X ~ n_samples x n_features: (100, 2) y ~ n_samples: (100,) First 5 samples: [[ 4.21850347 2.23419161] [ 0.90779887 0.45984362] [-0.27652528 5.08127768] [ 0.08848433 2.32299086] [ 3.24329731 1.21460627]] First 5 labels: [1 1 0 0 1] . As the data is two-dimensional, we can plot each sample as a point in a two-dimensional coordinate system, with the first feature being the x-axis and the second feature being the y-axis. . plt.scatter(X[y == 0, 0], X[y == 0, 1], c=&#39;blue&#39;, s=40, label=&#39;0&#39;) plt.scatter(X[y == 1, 0], X[y == 1, 1], c=&#39;red&#39;, s=40, label=&#39;1&#39;, marker=&#39;s&#39;) plt.xlabel(&#39;first feature&#39;) plt.ylabel(&#39;second feature&#39;) plt.legend(loc=&#39;upper right&#39;); . Classification is a supervised task, and since we are interested in its performance on unseen data, we split our data into two parts: . a training set that the learning algorithm uses to fit the model | a test set to evaluate the generalization performance of the model | The train_test_split function from the model_selection module does that for us -- we will use it to split a dataset into 75% training data and 25% test data. . . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234, stratify=y) . The scikit-learn estimator API . . Every algorithm is exposed in scikit-learn via an &#39;&#39;Estimator&#39;&#39; object. (All models in scikit-learn have a very consistent interface). For instance, we first import the logistic regression class. . from sklearn.linear_model import LogisticRegression . Next, we instantiate the estimator object. . classifier = LogisticRegression() . X_train.shape . (75, 2) . y_train.shape . (75,) . To built the model from our data, that is to learn how to classify new points, we call the fit function with the training data, and the corresponding training labels (the desired output for the training data point): . classifier.fit? . Signature: classifier.fit(X, y, sample_weight=None) Docstring: Fit the model according to the given training data. Parameters - X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,) default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.17 *sample_weight* support to LogisticRegression. Returns - self Fitted estimator. Notes -- The SAGA solver supports both float64 and float32 bit arrays. File: c: users babib anaconda3 envs mne lib site-packages sklearn linear_model _logistic.py Type: method . classifier.fit(X_train, y_train) . LogisticRegression() . (Some estimator methods such as fit return self by default. Thus, after executing the code snippet above, you will see the default parameters of this particular instance of LogisticRegression. Another way of retrieving the estimator&#39;s ininitialization parameters is to execute classifier.get_params(), which returns a parameter dictionary.) . classifier.coef_ . array([[ 0.87015709, -2.23877721]]) . classifier.get_params() . {&#39;C&#39;: 1.0, &#39;class_weight&#39;: None, &#39;dual&#39;: False, &#39;fit_intercept&#39;: True, &#39;intercept_scaling&#39;: 1, &#39;l1_ratio&#39;: None, &#39;max_iter&#39;: 100, &#39;multi_class&#39;: &#39;auto&#39;, &#39;n_jobs&#39;: None, &#39;penalty&#39;: &#39;l2&#39;, &#39;random_state&#39;: None, &#39;solver&#39;: &#39;lbfgs&#39;, &#39;tol&#39;: 0.0001, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False} . We can then apply the model to unseen data and use the model to predict the estimated outcome using the predict method: . prediction = classifier.predict(X_test) . We can compare these against the true labels: . print(prediction) print(y_test) . [1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0] [1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0] . We can evaluate our classifier quantitatively by measuring what fraction of predictions is correct. This is called accuracy: . prediction == y_test . array([ True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True]) . np.mean(prediction == y_test) . 0.84 . There is also a convenience function , score, that all scikit-learn classifiers have to compute this directly from the test data: . classifier.score(X_test, y_test) . 0.84 . It is often helpful to compare the generalization performance (on the test set) to the performance on the training set: . classifier.score(X_train, y_train) . 0.9733333333333334 . LogisticRegression is a so-called linear model, that means it will create a decision that is linear in the input space. In 2d, this simply means it finds a line to separate the blue from the red: . from figures import plot_2d_separator plt.scatter(X[y == 0, 0], X[y == 0, 1], c=&#39;blue&#39;,s=40, label=&#39;0&#39;) plt.scatter(X[y == 1, 0], X[y == 1, 1], c=&#39;red&#39;, s=40, label=&#39;1&#39;, marker=&#39;s&#39;) plt.xlabel(&quot;first feature&quot;) plt.ylabel(&quot;second feature&quot;) plot_2d_separator(classifier, X) plt.legend(loc=&#39;upper right&#39;); . Estimated parameters: All the estimated model parameters are attributes of the estimator object ending by an underscore. Here, these are the coefficients and the offset of the line: . print(classifier.coef_) print(classifier.intercept_) . [[ 0.87015709 -2.23877721]] [4.64737766] . K Nearest Neighbors (KNN) . Another popular and easy to understand classifier is K nearest neighbors (kNN). It has one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class. . The KNN classifier is a non-parametric classifier that simply stores the training data $ mathcal{D}$ and classifies each new instance $x$ using a majority vote over its&#39; set of $K$ nearest neighbors $ mathcal{N}_K(x)$ computed using any distance function $d: R^D times mathbb{R}^D rightarrow mathbb{R} $. . KNN Classification Function: . $$g_{KNN}(x) = argmax_{y in mathcal{Y}} sum_{i in mathcal{N}_K(x)} mathbb{I}[y_i=y]$$ . Use of KNN requires choosing the distance function $d$ and the number of neighbors $K$. . . . In general, KNN can work with any distance function $d$ satisfying non-negativity $d( bf{x}, bf{x}&#39;) geq 0$ and identity of indiscernibles $d( bf{x}, bf{x})=0$. . | Alternatively, KNN can work with any similarity function $s$ satisfying non-negativity $s( bf{x}, bf{y}) geq 0$ that attains it&#39;s maximum on indiscernibles $s( bf{x}, bf{x})= max_{ bf{x}&#39;} s( bf{x}, bf{x}&#39;)$. . | . However, the more structure the distance or similarity function has (symmetry, triangle inequality), the more structure you can exploit when designing algorithms. | . Minkowski Distance ($ ell_p$ norms)} . Given two data vectors $ bf{x}, bf{x}&#39; in mathbb{R}^D$, the Minkowski Distance with parameter $p$ (the $ ell_p$ norm) is a proper metric defined as follows: . begin{align*} d_p( bf{x}, bf{x}&#39;) &amp;= || bf{x}- bf{x}&#39;||_p &amp;= left( sum_{i=1}^D |x_d-x&#39;_d|^p right)^{1/p} end{align*}Special cases include Euclidean distance ($p=2$), Manhattan distance ($p=1$) and Chebyshev distance ($p= infty$). . Brute Force KNN . Given any distance function $d$, brute force KNN works by computing the distance $d_i = d( bf{x}_i, bf{x}_*)$ from a target point $ bf{x}_*$ to all of the training points $ bf{x}_i$. . | You then simply sort the distances $ {d_i,i=1:N }$ and choose the data cases with the $K$ smallest distances to form the neighbor set $ mathcal{N}_K( bf{x}_*)$. Using a similarity function is identical, but you select the $K$ most similar data cases. . | Once the $K$ neighbors are selected, applying the classification rule is easy. . | . In Sklearn the KNN interface is exactly the same as for LogisticRegression above. . from sklearn.neighbors import KNeighborsClassifier . This time we set a parameter of the KNeighborsClassifier to tell it we only want to look at one nearest neighbor: . knn = KNeighborsClassifier(n_neighbors=1) . We fit the model with out training data . knn.fit(X_train, y_train) . KNeighborsClassifier(n_neighbors=1) . plt.scatter(X[y == 0, 0], X[y == 0, 1], c=&#39;blue&#39;, s=40, label=&#39;0&#39;) plt.scatter(X[y == 1, 0], X[y == 1, 1], c=&#39;red&#39;, s=40, label=&#39;1&#39;, marker=&#39;s&#39;) plt.xlabel(&quot;first feature&quot;) plt.ylabel(&quot;second feature&quot;) plot_2d_separator(knn, X) plt.legend(loc=&#39;upper right&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-14T19:49:43.120164 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ knn.score(X_test, y_test) . 1.0 . EXERCISE: Apply the KNeighborsClassifier to the ``iris`` dataset. Play with different values of the ``n_neighbors`` and observe how training and test score change. | . # %load solutions/05A_knn_with_diff_k.py from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split iris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234, stratify=y) X_trainsub, X_valid, y_trainsub, y_valid = train_test_split(X_train, y_train, test_size=0.5, random_state=1234, stratify=y_train) for k in range(1, 20): knn = KNeighborsClassifier(n_neighbors=k) train_score = knn.fit(X_trainsub, y_trainsub). score(X_trainsub, y_trainsub) valid_score = knn.score(X_valid, y_valid) print(&#39;k: %d, Train/Valid Acc: %.3f/%.3f&#39; % (k, train_score, valid_score)) knn = KNeighborsClassifier(n_neighbors=9) knn.fit(X_train, y_train) print(&#39;k=9 Test Acc: %.3f&#39; % knn.score(X_test, y_test)) . k: 1, Train/Valid Acc: 1.000/0.946 k: 2, Train/Valid Acc: 1.000/0.964 k: 3, Train/Valid Acc: 1.000/0.946 k: 4, Train/Valid Acc: 1.000/0.964 k: 5, Train/Valid Acc: 1.000/0.929 k: 6, Train/Valid Acc: 1.000/0.929 k: 7, Train/Valid Acc: 1.000/0.929 k: 8, Train/Valid Acc: 1.000/0.929 k: 9, Train/Valid Acc: 1.000/0.929 k: 10, Train/Valid Acc: 1.000/0.946 k: 11, Train/Valid Acc: 1.000/0.946 k: 12, Train/Valid Acc: 1.000/0.964 k: 13, Train/Valid Acc: 1.000/0.929 k: 14, Train/Valid Acc: 1.000/0.946 k: 15, Train/Valid Acc: 1.000/0.929 k: 16, Train/Valid Acc: 1.000/0.929 k: 17, Train/Valid Acc: 1.000/0.929 k: 18, Train/Valid Acc: 0.964/0.946 k: 19, Train/Valid Acc: 0.964/0.929 k=9 Test Acc: 0.974 . %matplotlib inline import matplotlib.pyplot as plt import numpy as np . Supervised Learning -- Regression Analysis . In regression we are trying to predict a continuous output variable -- in contrast to the nominal variables we were predicting in the previous classification examples. . Let&#39;s start with a simple toy example with one feature dimension (explanatory variable) and one target variable. We will create a dataset out of a sine curve with some noise: . x = np.linspace(-3, 3, 100) print(x) . [-3. -2.93939394 -2.87878788 -2.81818182 -2.75757576 -2.6969697 -2.63636364 -2.57575758 -2.51515152 -2.45454545 -2.39393939 -2.33333333 -2.27272727 -2.21212121 -2.15151515 -2.09090909 -2.03030303 -1.96969697 -1.90909091 -1.84848485 -1.78787879 -1.72727273 -1.66666667 -1.60606061 -1.54545455 -1.48484848 -1.42424242 -1.36363636 -1.3030303 -1.24242424 -1.18181818 -1.12121212 -1.06060606 -1. -0.93939394 -0.87878788 -0.81818182 -0.75757576 -0.6969697 -0.63636364 -0.57575758 -0.51515152 -0.45454545 -0.39393939 -0.33333333 -0.27272727 -0.21212121 -0.15151515 -0.09090909 -0.03030303 0.03030303 0.09090909 0.15151515 0.21212121 0.27272727 0.33333333 0.39393939 0.45454545 0.51515152 0.57575758 0.63636364 0.6969697 0.75757576 0.81818182 0.87878788 0.93939394 1. 1.06060606 1.12121212 1.18181818 1.24242424 1.3030303 1.36363636 1.42424242 1.48484848 1.54545455 1.60606061 1.66666667 1.72727273 1.78787879 1.84848485 1.90909091 1.96969697 2.03030303 2.09090909 2.15151515 2.21212121 2.27272727 2.33333333 2.39393939 2.45454545 2.51515152 2.57575758 2.63636364 2.6969697 2.75757576 2.81818182 2.87878788 2.93939394 3. ] . rng = np.random.RandomState(42) y = np.sin(4 * x) + x + rng.uniform(size=len(x)) . plt.plot(x, y, &#39;o&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-14T19:50:03.210166 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ Linear Regression . The first model that we will introduce is the so-called simple linear regression. Here, we want to fit a line to the data, which . One of the simplest models again is a linear one, that simply tries to predict the data as lying on a line. One way to find such a line is LinearRegression (also known as Ordinary Least Squares (OLS) regression). The interface for LinearRegression is exactly the same as for the classifiers before, only that y now contains float values, instead of classes. . As we remember, the scikit-learn API requires us to provide the target variable (y) as a 1-dimensional array; scikit-learn&#39;s API expects the samples (X) in form a 2-dimensional array -- even though it may only consist of 1 feature. Thus, let us convert the 1-dimensional x NumPy array into an X array with 2 axes: . print(&#39;Before: &#39;, x.shape) X = x[:, np.newaxis] print(&#39;After: &#39;, X.shape) . Before: (100,) After: (100, 1) . Again, we start by splitting our dataset into a training (75%) and a test set (25%): . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) . Next, we use the learning algorithm implemented in LinearRegression to fit a regression model to the training data: . from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X_train, y_train) . LinearRegression() . After fitting to the training data, we paramerterized a linear regression model with the following values. . print(&#39;Weight coefficients: &#39;, regressor.coef_) print(&#39;y-axis intercept: &#39;, regressor.intercept_) . Weight coefficients: [0.90211711] y-axis intercept: 0.44840974988268 . Since our regression model is a linear one, the relationship between the target variable (y) and the feature variable (x) is defined as . $$y = weight times x + text{intercept}$$. . Plugging in the min and max values into thos equation, we can plot the regression fit to our training data: . min_pt = X.min() * regressor.coef_[0] + regressor.intercept_ max_pt = X.max() * regressor.coef_[0] + regressor.intercept_ print(min_pt, max_pt, X.min(), X.max()) . -2.2579415855468374 3.154761085312198 -3.0 3.0 . plt.plot([X.min(), X.max()], [min_pt, max_pt]) plt.plot(X_train, y_train, &#39;o&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-14T19:50:18.828487 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ Similar to the estimators for classification in the previous notebook, we use the predict method to predict the target variable. And we expect these predicted values to fall onto the line that we plotted previously: . y_pred_train = regressor.predict(X_train) . plt.plot(X_train, y_train, &#39;o&#39;, label=&quot;data&quot;) plt.plot(X_train, y_pred_train, &#39;o&#39;, label=&quot;prediction&quot;) plt.plot([X.min(), X.max()], [min_pt, max_pt], label=&#39;fit&#39;) plt.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x18eb141f520&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-14T19:50:24.565745 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ As we can see in the plot above, the line is able to capture the general slope of the data, but not many details. . Next, let&#39;s try the test set: . y_pred_test = regressor.predict(X_test) . print(X_test.shape) print(y_pred_test.shape) . (25, 1) (25,) . plt.plot(X_test, y_test, &#39;o&#39;, label=&quot;data&quot;) plt.plot(X_test, y_pred_test, &#39;o&#39;, label=&quot;prediction&quot;) plt.plot([X.min(), X.max()], [min_pt, max_pt], label=&#39;fit&#39;) plt.legend(loc=&#39;best&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-14T19:50:34.079480 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ Again, scikit-learn provides an easy way to evaluate the prediction quantitatively using the score method. For regression tasks, this is the R2 score. Another popular way would be the Mean Squared Error (MSE). As its name implies, the MSE is simply the average squared difference over the predicted and actual target values . $$MSE = frac{1}{n} sum_{i=1}^{n} ( text{predicted}_i - text{true}_i)^2$$ . regressor.score(X_test, y_test) . 0.7994321405079685 . EXERCISE: Add a feature containing `sin(4x)` to `X` and redo the fit. Visualize the predictions with this new richer, yet linear, model. | . # %load solutions/06B_lin_with_sine.py . KNeighborsRegression . As for classification, we can also use a neighbor based method for regression. We can simply take the output of the nearest point, or we could average several nearest points. This method is less popular for regression than for classification, but still a good baseline. . from sklearn.neighbors import KNeighborsRegressor kneighbor_regression = KNeighborsRegressor(n_neighbors=1) kneighbor_regression.fit(X_train, y_train) . KNeighborsRegressor(n_neighbors=1) . Again, let us look at the behavior on training and test set: . y_pred_train = kneighbor_regression.predict(X_train) plt.plot(X_train, y_train, &#39;o&#39;, label=&quot;data&quot;, markersize=10) plt.plot(X_train, y_pred_train, &#39;s&#39;, label=&quot;prediction&quot;, markersize=4) plt.legend(loc=&#39;best&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-10-14T19:50:42.891786 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ On the training set, we do a perfect job: each point is its own nearest neighbor! . y_pred_test = kneighbor_regression.predict(X_test) plt.plot(X_test, y_test, &#39;o&#39;, label=&quot;data&quot;, markersize=8) plt.plot(X_test, y_pred_test, &#39;s&#39;, label=&quot;prediction&quot;, markersize=4) plt.legend(loc=&#39;best&#39;); . On the test set, we also do a better job of capturing the variation, but our estimates look much messier than before. Let us look at the R2 score: . kneighbor_regression.score(X_test, y_test) . 0.9166293022467948 . Much better than before! Here, the linear model was not a good fit for our problem; it was lacking in complexity and thus under-fit our data. . EXERCISE: Compare the KNeighborsRegressor and LinearRegression on the boston housing dataset. You can load the dataset using ``sklearn.datasets.load_boston``. You can learn about the dataset by reading the ``DESCR`` attribute. | . # %load solutions/06A_knn_vs_linreg.py . Cross-Validation and scoring methods . In the previous sections and notebooks, we split our dataset into two parts, a training set and a test set. We used the training set to fit our model, and we used the test set to evaluate its generalization performance -- how well it performs on new, unseen data. . . However, often (labeled) data is precious, and this approach lets us only use ~ 3/4 of our data for training. On the other hand, we will only ever try to apply our model 1/4 of our data for testing. A common way to use more of the data to build a model, but also get a more robust estimate of the generalization performance, is cross-validation. In cross-validation, the data is split repeatedly into a training and non-overlapping test-sets, with a separate model built for every pair. The test-set scores are then aggregated for a more robust estimate. . The most common way to do cross-validation is k-fold cross-validation, in which the data is first split into k (often 5 or 10) equal-sized folds, and then for each iteration, one of the k folds is used as test data, and the rest as training data: . . This way, each data point will be in the test-set exactly once, and we can use all but a k&#39;th of the data for training. Let us apply this technique to evaluate the KNeighborsClassifier algorithm on the Iris dataset: . from sklearn.datasets import load_iris from sklearn.neighbors import KNeighborsClassifier iris = load_iris() X, y = iris.data, iris.target classifier = KNeighborsClassifier() . The labels in iris are sorted, which means that if we split the data as illustrated above, the first fold will only have the label 0 in it, while the last one will only have the label 2: . y . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) . To avoid this problem in evaluation, we first shuffle our data: . import numpy as np rng = np.random.RandomState(0) permutation = rng.permutation(len(X)) X, y = X[permutation], y[permutation] print(y) . [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 1 1 1 2 0 2 0 0 1 2 2 2 2 1 2 1 1 2 2 2 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0 1 0 2 1 0 1 2 1 0 2 2 2 2 0 0 2 2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 1 1 0 1 2 2 0 1 1 1 1 0 0 0 2 1 2 0] . Now implementing cross-validation is easy: . k = 5 n_samples = len(X) fold_size = n_samples // k scores = [] masks = [] for fold in range(k): # generate a boolean mask for the test set in this fold test_mask = np.zeros(n_samples, dtype=bool) test_mask[fold * fold_size : (fold + 1) * fold_size] = True # store the mask for visualization masks.append(test_mask) # create training and test sets using this mask X_test, y_test = X[test_mask], y[test_mask] X_train, y_train = X[~test_mask], y[~test_mask] # fit the classifier classifier.fit(X_train, y_train) # compute the score and record it scores.append(classifier.score(X_test, y_test)) . Let&#39;s check that our test mask does the right thing: . import matplotlib.pyplot as plt %matplotlib inline plt.matshow(masks, cmap=&#39;gray_r&#39;) . &lt;matplotlib.image.AxesImage at 0x2435599cf40&gt; . And now let&#39;s look a the scores we computed: . print(scores) . [0.9666666666666667, 0.9, 1.0, 1.0, 0.9333333333333333] . print(np.mean(scores)) . 0.96 . As you can see, there is a rather wide spectrum of scores from 90% correct to 100% correct. If we only did a single split, we might have gotten either answer. . As cross-validation is such a common pattern in machine learning, there are functions to do the above for you with much more flexibility and less code. The sklearn.model_selection module has all functions related to cross validation. There easiest function is cross_val_score which takes an estimator and a dataset, and will do all of the splitting for you: . from sklearn.model_selection import cross_val_score scores = cross_val_score(classifier, X, y) print(scores) print(np.mean(scores)) . [1. 0.93333333 1. 1. 0.93333333] 0.9733333333333334 . As you can see, the function uses three folds by default. You can change the number of folds using the cv argument: . cross_val_score(classifier, X, y, cv=5) . array([1. , 0.93333333, 1. , 1. , 0.93333333]) . There are also helper objects in the cross-validation module that will generate indices for you for all kinds of different cross-validation methods, including k-fold: . from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit . By default, cross_val_score will use StratifiedKFold for classification, which ensures that the class proportions in the dataset are reflected in each fold. If you have a binary classification dataset with 90% of data point belonging to class 0, that would mean that in each fold, 90% of datapoints would belong to class 0. If you would just use KFold cross-validation, it is likely that you would generate a split that only contains class 0. It is generally a good idea to use StratifiedKFold whenever you do classification. . StratifiedKFold would also remove our need to shuffle iris. Let&#39;s see what kinds of folds it generates on the unshuffled iris dataset. Each cross-validation class is a generator of sets of training and test indices: . cv = StratifiedKFold(n_splits=5) for train, test in cv.split(iris.data, iris.target): print(test) . [ 0 1 2 3 4 5 6 7 8 9 50 51 52 53 54 55 56 57 58 59 100 101 102 103 104 105 106 107 108 109] [ 10 11 12 13 14 15 16 17 18 19 60 61 62 63 64 65 66 67 68 69 110 111 112 113 114 115 116 117 118 119] [ 20 21 22 23 24 25 26 27 28 29 70 71 72 73 74 75 76 77 78 79 120 121 122 123 124 125 126 127 128 129] [ 30 31 32 33 34 35 36 37 38 39 80 81 82 83 84 85 86 87 88 89 130 131 132 133 134 135 136 137 138 139] [ 40 41 42 43 44 45 46 47 48 49 90 91 92 93 94 95 96 97 98 99 140 141 142 143 144 145 146 147 148 149] . As you can see, there are a couple of samples from the beginning, then from the middle, and then from the end, in each of the folds. This way, the class ratios are preserved. Let&#39;s visualize the split: . def plot_cv(cv, features, labels): masks = [] for train, test in cv.split(features, labels): mask = np.zeros(len(labels), dtype=bool) mask[test] = 1 masks.append(mask) plt.matshow(masks, cmap=&#39;gray_r&#39;) . plot_cv(StratifiedKFold(n_splits=5), iris.data, iris.target) . For comparison, again the standard KFold, that ignores the labels: . plot_cv(KFold(n_splits=5), iris.data, iris.target) . Keep in mind that increasing the number of folds will give you a larger training dataset, but will lead to more repetitions, and therefore a slower evaluation: . plot_cv(KFold(n_splits=10), iris.data, iris.target) . Another helpful cross-validation generator is ShuffleSplit. This generator simply splits of a random portion of the data repeatedly. This allows the user to specify the number of repetitions and the training set size independently: . plot_cv(ShuffleSplit(n_splits=5, test_size=.2), iris.data, iris.target) . If you want a more robust estimate, you can just increase the number of splits: . plot_cv(ShuffleSplit(n_splits=20, test_size=.2), iris.data, iris.target) . You can use all of these cross-validation generators with the cross_val_score method: . cv = ShuffleSplit(n_splits=5, test_size=.2) cross_val_score(classifier, X, y, cv=cv) . array([0.93333333, 0.93333333, 0.96666667, 0.93333333, 1. ]) . EXERCISE: Perform three-fold cross-validation using the ``KFold`` class on the iris dataset without shuffling the data. Can you explain the result? | . # %load solutions/13_cross_validation.py cv = KFold(n_splits=3) cross_val_score(classifier, iris.data, iris.target, cv=cv) . A recap on Scikit-learn&#39;s estimator interface . Scikit-learn strives to have a uniform interface across all methods. Given a scikit-learn estimator object named model, the following methods are available (not all for each model): . Available in all Estimators model.fit() : fit training data. For supervised learning applications, this accepts two arguments: the data X and the labels y (e.g. model.fit(X, y)). For unsupervised learning applications, fit takes only a single argument, the data X (e.g. model.fit(X)). | . | Available in supervised estimators . model.predict() : given a trained model, predict the label of a new set of data. This method accepts one argument, the new data X_new (e.g. model.predict(X_new)), and returns the learned label for each object in the array. | model.predict_proba() : For classification problems, some estimators also provide this method, which returns the probability that a new observation has each categorical label. In this case, the label with the highest probability is returned by model.predict(). | model.decision_function() : For classification problems, some estimators provide an uncertainty estimate that is not a probability. For binary classification, a decision_function &gt;= 0 means the positive class will be predicted, while &lt; 0 means the negative class. | model.score() : for classification or regression problems, most (all?) estimators implement a score method. Scores are between 0 and 1, with a larger score indicating a better fit. For classifiers, the score method computes the prediction accuracy. For regressors, score computes the coefficient of determination (R2) of the prediction. | model.transform() : For feature selection algorithms, this will reduce the dataset to the selected features. For some classification and regression models such as some linear models and random forests, this method reduces the dataset to the most informative features. These classification and regression models can therefore also be used as feature selection methods. | . | Available in unsupervised estimators . model.transform() : given an unsupervised model, transform new data into the new basis. This also accepts one argument X_new, and returns the new representation of the data based on the unsupervised model. | model.fit_transform() : some estimators implement this method, which more efficiently performs a fit and a transform on the same input data. | model.predict() : for clustering algorithms, the predict method will produce cluster labels for new data points. Not all clustering methods have this functionality. | model.predict_proba() : Gaussian mixture models (GMMs) provide the probability for each point to be generated by a given mixture component. | model.score() : Density models like KDE and GMMs provide the likelihood of the data under the model. | . | . Apart from fit, the two most important functions are arguably predict to produce a target variable (a y) transform, which produces a new representation of the data (an X). The following table shows for which class of models which function applies: . ``model.predict`model.transform`` . Classification | Preprocessing | . Regression | Dimensionality Reduction | . Clustering | Feature Extraction | . &nbsp; | Feature Selection | .",
            "url": "https://berdakh.github.io/blog/eeg/jupyter/2020/10/20/ScikitLearn-tutorial-part3.html",
            "relUrl": "/eeg/jupyter/2020/10/20/ScikitLearn-tutorial-part3.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to the scikit-learn -- machine learning in Python (part 2)",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;RZs0noGmPak&#39;, width=700, height=400) . Machine learning is about fitting models to data; for that reason, we&#39;ll start by discussing how data can be represented in order to be understood by the computer. Along with this, we&#39;ll build on our matplotlib examples from the previous section and show some examples of how to visualize data. . Data in scikit-learn . Data in scikit-learn, with very few exceptions, is assumed to be stored as a two-dimensional array, of shape [n_samples, n_features]. Many algorithms also accept scipy.sparse matrices of the same shape. . n_samples: The number of samples: each sample is an item to process (e.g. classify). A sample can be a document, a picture, a sound, a video, an astronomical object, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits. | n_features: The number of features or distinct traits that can be used to describe each item in a quantitative manner. Features are generally real-valued, but may be Boolean or discrete-valued in some cases. | . The number of features must be fixed in advance. However it can be very high dimensional (e.g. millions of features) with most of them being &quot;zeros&quot; for a given sample. This is a case where scipy.sparse matrices can be useful, in that they are much more memory-efficient than NumPy arrays. . As we recall from the previous section (or Jupyter notebook), we represent samples (data points or instances) as rows in the data array, and we store the corresponding features, the &quot;dimensions,&quot; as columns. . A Simple Example: the Iris Dataset . As an example of a simple dataset, we&#39;re going to take a look at the iris data stored by scikit-learn. The data consists of measurements of three different iris flower species. There are three different species of iris in this particular dataset as illustrated below: . Iris Setosa . Iris Versicolor . Iris Virginica . Quick Question: . Let&#39;s assume that we are interested in categorizing new observations; we want to predict whether unknown flowers are Iris-Setosa, Iris-Versicolor, or Iris-Virginica flowers, respectively. Based on what we&#39;ve discussed in the previous section, how would we construct such a dataset?* . Remember: we need a 2D array of size [n_samples x n_features]. . What would the n_samples refer to? . | What might the n_features refer to? . | . Remember that there must be a fixed number of features for each sample, and feature number j must be a similar kind of quantity for each sample. . Loading the Iris Data with Scikit-learn . For future experiments with machine learning algorithms, we recommend you to bookmark the UCI machine learning repository, which hosts many of the commonly used datasets that are useful for benchmarking machine learning algorithms -- a very popular resource for machine learning practioners and researchers. Conveniently, some of these datasets are already included in scikit-learn so that we can skip the tedious parts of downloading, reading, parsing, and cleaning these text/CSV files. You can find a list of available datasets in scikit-learn at: http://scikit-learn.org/stable/datasets/#toy-datasets. . For example, scikit-learn has a very straightforward set of data on these iris species. The data consist of the following: . Features in the Iris dataset: . sepal length in cm | sepal width in cm | petal length in cm | petal width in cm | | Target classes to predict: . Iris Setosa | Iris Versicolour | Iris Virginica | | . . (Image: &quot;Petal-sepal&quot;. Licensed under CC BY-SA 3.0 via Wikimedia Commons - https://commons.wikimedia.org/wiki/File:Petal-sepal.jpg#/media/File:Petal-sepal.jpg) . scikit-learn embeds a copy of the iris CSV file along with a helper function to load it into numpy arrays: . from sklearn.datasets import load_iris iris = load_iris() . The resulting dataset is a Bunch object: you can see what&#39;s available using the method keys(): . iris.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;]) . The features of each sample flower are stored in the data attribute of the dataset: . n_samples, n_features = iris.data.shape print(&#39;Number of samples:&#39;, n_samples) print(&#39;Number of features:&#39;, n_features) # the sepal length, sepal width, petal length and petal width of the first sample (first flower) print(iris.data[0]) . Number of samples: 150 Number of features: 4 [5.1 3.5 1.4 0.2] . The information about the class of each sample is stored in the target attribute of the dataset: . print(iris.data.shape) print(iris.target.shape) type(iris) . (150, 4) (150,) . sklearn.utils.Bunch . print(iris.target) . [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] . Iris Setosa Iris Versicolour Iris Virginica import numpy as np np.bincount(iris.target) . array([50, 50, 50], dtype=int64) . Using the NumPy&#39;s bincount function (above), we can see that the classes are distributed uniformly in this dataset - there are 50 flowers from each species, where . class 0: Iris-Setosa | class 1: Iris-Versicolor | class 2: Iris-Virginica | . These class names are stored in the last attribute, namely target_names: . print(iris.target_names) . [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] . This data is four dimensional, but we can visualize one or two of the dimensions at a time using a simple histogram or scatter-plot. Again, we&#39;ll start by enabling matplotlib inline mode: . %matplotlib inline import matplotlib.pyplot as plt . x_index = 3 colors = [&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;] for label, color in zip(range(len(iris.target_names)), colors): plt.hist(iris.data[iris.target==label, x_index], label=iris.target_names[label], color=color) plt.xlabel(iris.feature_names[x_index]) plt.legend(loc=&#39;upper right&#39;) plt.show() . x_index = 3 y_index = 1 colors = [&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;] for label, color in zip(range(len(iris.target_names)), colors): plt.scatter(iris.data[iris.target==label, x_index], iris.data[iris.target==label, y_index], label=iris.target_names[label], c=color) plt.xlabel(iris.feature_names[x_index]) plt.ylabel(iris.feature_names[y_index]) plt.legend(loc=&#39;upper left&#39;) plt.show() . EXERCISE: **Change** `x_index` **and** `y_index` **in the above script and find a combination of two parameters which maximally separate the three classes.** | This exercise is a preview of **dimensionality reduction**, which we&#39;ll see later. | . An aside: scatterplot matrices . Instead of looking at the data one plot at a time, a common tool that analysts use is called the scatterplot matrix. . Scatterplot matrices show scatter plots between all features in the data set, as well as histograms to show the distribution of each feature. . import pandas as pd iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) pd.plotting.scatter_matrix(iris_df, c=iris.target, figsize=(12, 12)); . Other Available Data . Scikit-learn makes available a host of datasets for testing learning algorithms. They come in three flavors: . Packaged Data: these small datasets are packaged with the scikit-learn installation, and can be downloaded using the tools in sklearn.datasets.load_* | Downloadable Data: these larger datasets are available for download, and scikit-learn includes tools which streamline this process. These tools can be found in sklearn.datasets.fetch_* | Generated Data: there are several datasets which are generated from models based on a random seed. These are available in the sklearn.datasets.make_* | . You can explore the available dataset loaders, fetchers, and generators using IPython&#39;s tab-completion functionality. After importing the datasets submodule from sklearn, type . datasets.load_&lt;TAB&gt; . or . datasets.fetch_&lt;TAB&gt; . or . datasets.make_&lt;TAB&gt; . to see a list of available functions. . from sklearn import datasets . Be warned: many of these datasets are quite large, and can take a long time to download! . If you start a download within the IPython notebook and you want to kill it, you can use ipython&#39;s &quot;kernel interrupt&quot; feature, available in the menu or using the shortcut Ctrl-m i. . You can press Ctrl-m h for a list of all ipython keyboard shortcuts. . Loading Digits Data . Now we&#39;ll take a look at another dataset, one where we have to put a bit more thought into how to represent the data. We can explore the data in a similar manner as above: . from sklearn.datasets import load_digits digits = load_digits() . digits.keys() . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;feature_names&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;]) . print(digits[&#39;DESCR&#39;]) . .. _digits_dataset: Optical recognition of handwritten digits dataset -- **Data Set Characteristics:** :Number of Instances: 5620 :Number of Attributes: 64 :Attribute Information: 8x8 image of integer pixels in the range 0..16. :Missing Attribute Values: None :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr) :Date: July; 1998 This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions. For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994. .. topic:: References - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University. - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika. - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005. - Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000. . n_samples, n_features = digits.data.shape print((n_samples, n_features)) . (1797, 64) . print(digits.data[0]) . [ 0. 0. 5. 13. 9. 1. 0. 0. 0. 0. 13. 15. 10. 15. 5. 0. 0. 3. 15. 2. 0. 11. 8. 0. 0. 4. 12. 0. 0. 8. 8. 0. 0. 5. 8. 0. 0. 9. 8. 0. 0. 4. 11. 0. 1. 12. 7. 0. 0. 2. 14. 5. 10. 12. 0. 0. 0. 0. 6. 13. 10. 0. 0. 0.] . print(digits.target) . [0 1 2 ... 8 9 8] . The target here is just the digit represented by the data. The data is an array of length 64... but what does this data mean? . There&#39;s a clue in the fact that we have two versions of the data array: data and images. Let&#39;s take a look at them: . print(digits.data.shape) print(digits.images.shape) . (1797, 64) (1797, 8, 8) . We can see that they&#39;re related by a simple reshaping: . import numpy as np print(np.all(digits.images.reshape((1797, 64)) == digits.data)) . True . Let&#39;s visualize the data. It&#39;s little bit more involved than the simple scatter-plot we used above, but we can do it rather quickly. . # set up the figure fig = plt.figure(figsize=(6, 6)) # figure size in inches fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) # plot the digits: each image is 8x8 pixels for i in range(64): ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[]) ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation=&#39;nearest&#39;) # label the image with the target value ax.text(0, 7, str(digits.target[i])) . We see now what the features mean. Each feature is a real-valued quantity representing the darkness of a pixel in an 8x8 image of a hand-written digit. . Even though each sample has data that is inherently two-dimensional, the data matrix flattens this 2D data into a single vector, which can be contained in one row of the data matrix. . EXERCISE: working with the faces dataset: Here we&#39;ll take a moment for you to explore the datasets yourself. Later on we&#39;ll be using the Olivetti faces dataset. Take a moment to fetch the data (about 1.4MB), and visualize the faces. You can copy the code used to visualize the digits above, and modify it for this data. | . from sklearn.datasets import fetch_olivetti_faces . # fetch the faces data . # Use a script like above to plot the faces image data. # hint: plt.cm.bone is a good colormap for this data . Solution: . # %load solutions/03A_faces_plot.py import matplotlib.pyplot as plt . faces = fetch_olivetti_faces() # set up the figure fig = plt.figure(figsize=(6, 6)) # figure size in inches fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) # plot the faces: for i in range(64): ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[]) ax.imshow(faces.images[i], cmap=plt.cm.bone, interpolation=&#39;nearest&#39;) . faces.keys() . dict_keys([&#39;data&#39;, &#39;images&#39;, &#39;target&#39;, &#39;DESCR&#39;]) . print(faces[&#39;DESCR&#39;]) . Modified Olivetti faces dataset. The original database was available from http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html The version retrieved here comes in MATLAB format from the personal web page of Sam Roweis: http://www.cs.nyu.edu/~roweis/ There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The original dataset consisted of 92 x 112, while the Roweis version consists of 64x64 images. . import numpy as np np.unique(faces[&#39;target&#39;]) . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]) . x = faces[&#39;data&#39;] y = faces[&#39;target&#39;] print(x.shape, y.shape) . (400, 4096) (400,) . print(y) . [ 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 21 21 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 23 23 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24 24 24 25 25 25 25 25 25 25 25 25 25 26 26 26 26 26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 28 28 28 28 28 28 28 28 28 28 29 29 29 29 29 29 29 29 29 29 30 30 30 30 30 30 30 30 30 30 31 31 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33 33 33 33 33 34 34 34 34 34 34 34 34 34 34 35 35 35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 36 36 36 37 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 38 39 39 39 39 39 39 39 39 39 39] . %matplotlib inline import matplotlib.pyplot as plt import numpy as np . Training and Testing Data . To evaluate how well our supervised models generalize, we can split our data into a training and a test set: . . import sklearn . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression iris = load_iris() X, y = iris.data, iris.target classifier = LogisticRegression() . Thinking about how machine learning is normally performed, the idea of a train/test split makes sense. Real world systems train on the data they have, and as other data comes in (from customers, sensors, or other sources) the classifier that was trained must predict on fundamentally new data. We can simulate this during training using a train/test split - the test data is a simulation of &quot;future data&quot; which will come into the system during production. . Specifically for iris, the 150 labels in iris are sorted, which means that if we split the data using a proportional split, this will result in fudamentally altered class distributions. For instance, if we&#39;d perform a common 2/3 training data and 1/3 test data split, our training dataset will only consists of flower classes 0 and 1 (Setosa and Versicolor), and our test set will only contain samples with class label 2 (Virginica flowers). . Under the assumption that all samples are independent of each other (in contrast time series data), we want to randomly shuffle the dataset before we split the dataset as illustrated above. . y.shape . (150,) . Now we need to split the data into training and testing. Luckily, this is a common pattern in machine learning and scikit-learn has a pre-built function to split data into training and testing sets for you. Here, we use 50% of the data as training, and 50% testing. 80% and 20% is another common split, but there are no hard and fast rules. The most important thing is to fairly evaluate your system on data it has not seen during training! . from sklearn.model_selection import train_test_split train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.5, test_size=0.5, random_state=123) print(&quot;Labels for training and testing data&quot;) print(&#39;Train sample labels : n&#39;, train_y) print(&#39;Test sample labels : n&#39;, test_y) . Labels for training and testing data Train sample labels : [1 1 0 2 2 0 0 1 1 2 0 0 1 0 1 2 0 2 0 0 1 0 0 1 2 1 1 1 0 0 1 2 0 0 1 1 1 2 1 1 1 2 0 0 1 2 2 2 2 0 1 0 1 1 0 1 2 1 2 2 0 1 0 2 2 1 1 2 2 1 0 1 1 2 2] Test sample labels : [1 2 2 1 0 2 1 0 0 1 2 0 1 2 2 2 0 0 1 0 0 2 0 2 0 0 0 2 2 0 2 2 0 0 1 1 2 0 0 1 1 0 2 2 2 2 2 1 0 0 2 0 0 1 1 1 1 2 1 2 0 2 1 0 0 2 1 2 2 0 1 1 2 0 2] . . Tip: Stratified Split . Especially for relatively small datasets, it&#39;s better to stratify the split. Stratification means that we maintain the original class proportion of the dataset in the test and training sets. For example, after we randomly split the dataset as shown in the previous code example, we have the following class proportions in percent: . print(&#39;All:&#39;, np.bincount(y) / float(len(y)) * 100.0) print(&#39;Training:&#39;, np.bincount(train_y) / float(len(train_y)) * 100.0) print(&#39;Test:&#39;, np.bincount(test_y) / float(len(test_y)) * 100.0) . All: [33.33333333 33.33333333 33.33333333] Training: [30.66666667 40. 29.33333333] Test: [36. 26.66666667 37.33333333] . So, in order to stratify the split, we can pass the label array as an additional option to the train_test_split function: . train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.5, test_size=0.5, random_state=123, stratify=y) print(&#39;All:&#39;, np.bincount(y) / float(len(y)) * 100.0) print(&#39;Training:&#39;, np.bincount(train_y) / float(len(train_y)) * 100.0) print(&#39;Test:&#39;, np.bincount(test_y) / float(len(test_y)) * 100.0) . All: [33.33333333 33.33333333 33.33333333] Training: [33.33333333 33.33333333 33.33333333] Test: [33.33333333 33.33333333 33.33333333] . . By evaluating our classifier performance on data that has been seen during training, we could get false confidence in the predictive power of our model. In the worst case, it may simply memorize the training samples but completely fails classifying new, similar samples -- we really don&#39;t want to put such a system into production! . Instead of using the same dataset for training and testing (this is called &quot;resubstitution evaluation&quot;), it is much much better to use a train/test split in order to estimate how well your trained model is doing on new data. . classifier.fit(train_X, train_y) . C: Users babib anaconda3 envs mne lib site-packages sklearn linear_model _logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . LogisticRegression() . pred_y = classifier.predict(test_X) . pred_y . array([0, 2, 1, 0, 2, 0, 1, 2, 0, 0, 2, 1, 2, 0, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 1, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 1, 1, 2, 0, 2, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0, 2, 0, 2, 1, 1, 2, 0, 2, 1, 0, 0, 1]) . print(test_X.shape) print(pred_y.shape) . (75, 4) (75,) . print(&quot;Fraction Correct [Accuracy]:&quot;) print(np.sum(pred_y == test_y) / float(len(test_y))) . Fraction Correct [Accuracy]: 0.9466666666666667 . We can also visualize the correct and failed predictions . print(&#39;Samples correctly classified:&#39;) correct_idx = np.where(pred_y == test_y)[0] print(correct_idx) . Samples correctly classified: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 45 46 47 48 50 51 53 54 55 56 57 58 59 61 62 63 64 65 66 67 68 69 70 71 72 73 74] . len(test_y) . 75 . print(&#39; nSamples incorrectly classified:&#39;) incorrect_idx = np.where(pred_y != test_y)[0] print(incorrect_idx) . Samples incorrectly classified: [44 49 52 60] . # Plot two dimensions colors = [&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;gray&quot;] for n, color in enumerate(colors): idx = np.where(test_y == n)[0] plt.scatter(test_X[idx, 1], test_X[idx, 2], color=color, label=&quot;Class %s&quot; % str(n)) plt.scatter(test_X[incorrect_idx, 1], test_X[incorrect_idx, 2], color=&quot;yellow&quot;) plt.xlabel(&#39;sepal width [cm]&#39;) plt.ylabel(&#39;petal length [cm]&#39;) plt.legend(loc=3) plt.title(&quot;Iris Classification results&quot;) plt.show() . We can see that the errors occur in the area where green (class 1) and gray (class 2) overlap. This gives us insight about what features to add - any feature which helps separate class 1 and class 2 should improve classifier performance. . EXERCISE: Print the true labels of 3 wrong predictions and modify the scatterplot code, which we used above, to visualize and distinguish these three samples with different markers in the 2D scatterplot. Can you explain why our classifier made these wrong predictions? | . # %load solutions/04_wrong-predictions.py for i in incorrect_idx: print(&#39;%d: Predicted %d True label %d&#39; % (i, pred_y[i], test_y[i])) # Plot two dimensions colors = [&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;gray&quot;] for n, color in enumerate(colors): idx = np.where(test_y == n)[0] plt.scatter(test_X[idx, 1], test_X[idx, 2], color=color, label=&quot;Class %s&quot; % str(n)) for i, marker in zip(incorrect_idx, [&#39;x&#39;, &#39;s&#39;, &#39;v&#39;]): plt.scatter(test_X[i, 1], test_X[i, 2], color=&quot;darkred&quot;, marker=marker, s=40, label=i) plt.xlabel(&#39;sepal width [cm]&#39;) plt.ylabel(&#39;petal length [cm]&#39;) plt.legend(loc=1, scatterpoints=1) plt.title(&quot;Iris Classification results&quot;) plt.show() . 44: Predicted 2 True label 1 49: Predicted 2 True label 1 52: Predicted 1 True label 2 60: Predicted 1 True label 2 .",
            "url": "https://berdakh.github.io/blog/eeg/jupyter/2020/10/19/ScikitLearn-tutorial-part2.html",
            "relUrl": "/eeg/jupyter/2020/10/19/ScikitLearn-tutorial-part2.html",
            "date": " • Oct 19, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Introduction to the scikit-learn -- machine learning in Python (part 1)",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;iJYszwtosSg&#39;, width=700, height=400) . What is Machine Learning? . Machine learning is the process of extracting knowledge from data automatically, usually with the goal of making predictions on new, unseen data. A classical example is a spam filter, for which the user keeps labeling incoming mails as either spam or not spam. A machine learning algorithm then &quot;learns&quot; a predictive model from data that distinguishes spam from normal emails, a model which can predict for new emails whether they are spam or not. . Central to machine learning is the concept of automating decision making from data without the user specifying explicit rules how this decision should be made. . For the case of emails, the user doesn&#39;t provide a list of words or characteristics that make an email spam. Instead, the user provides examples of spam and non-spam emails that are labeled as such. . The second central concept is generalization. The goal of a machine learning model is to predict on new, previously unseen data. In a real-world application, we are not interested in marking an already labeled email as spam or not. Instead, we want to make the user&#39;s life easier by automatically classifying new incoming mail. . . The data is presented to the algorithm usually as a two-dimensional array (or matrix) of numbers. Each data point (also known as a sample or training instance) that we want to either learn from or make a decision on is represented as a list of numbers, a so-called feature vector, and its containing features represent the properties of this point. . Later, we will work with a popular dataset called Iris -- among many other datasets. Iris, a classic benchmark dataset in the field of machine learning, contains the measurements of 150 iris flowers from 3 different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica. . Iris Setosa . Iris Versicolor . Iris Virginica . We represent each flower sample as one row in our data array, and the columns (features) represent the flower measurements in centimeters. For instance, we can represent this Iris dataset, consisting of 150 samples and 4 features, a 2-dimensional array or matrix $ mathbb{R}^{150 times 4}$ in the following format: . mathbf{X} = begin{bmatrix} x_{1}^{(1)} &amp; x_{2}^{(1)} &amp; x_{3}^{(1)} &amp; dots &amp; x_{4}^{(1)} x_{1}^{(2)} &amp; x_{2}^{(2)} &amp; x_{3}^{(2)} &amp; dots &amp; x_{4}^{(2)} vdots &amp; vdots &amp; vdots &amp; ddots &amp; vdots x_{1}^{(150)} &amp; x_{2}^{(150)} &amp; x_{3}^{(150)} &amp; dots &amp; x_{4}^{(150)} end{bmatrix} . (The superscript denotes the ith row, and the subscript denotes the jth feature, respectively. . There are two kinds of machine learning we will talk about today: supervised learning and unsupervised learning. . Supervised Learning: Classification and regression . In Supervised Learning, we have a dataset consisting of both input features and a desired output, such as in the spam / no-spam example. The task is to construct a model (or program) which is able to predict the desired output of an unseen object given the set of features. . Some more complicated examples are: . Given a multicolor image of an object through a telescope, determine whether that object is a star, a quasar, or a galaxy. | Given a photograph of a person, identify the person in the photo. | Given a list of movies a person has watched and their personal rating of the movie, recommend a list of movies they would like. | Given a persons age, education and position, infer their salary | . What these tasks have in common is that there is one or more unknown quantities associated with the object which needs to be determined from other observed quantities. . Supervised learning is further broken down into two categories, classification and regression: . In classification, the label is discrete, such as &quot;spam&quot; or &quot;no spam&quot;. In other words, it provides a clear-cut distinction between categories. Furthermore, it is important to note that class labels are nominal, not ordinal variables. Nominal and ordinal variables are both subcategories of categorical variable. Ordinal variables imply an order, for example, T-shirt sizes &quot;XL &gt; L &gt; M &gt; S&quot;. On the contrary, nominal variables don&#39;t imply an order, for example, we (usually) can&#39;t assume &quot;orange &gt; blue &gt; green&quot;. | . In regression, the label is continuous, that is a float output. For example, in astronomy, the task of determining whether an object is a star, a galaxy, or a quasar is a classification problem: the label is from three distinct categories. On the other hand, we might wish to estimate the age of an object based on such observations: this would be a regression problem, because the label (age) is a continuous quantity. | . In supervised learning, there is always a distinction between a training set for which the desired outcome is given, and a test set for which the desired outcome needs to be inferred. The learning model fits the predictive model to the training set, and we use the test set to evaluate its generalization performance. . Unsupervised Learning . In Unsupervised Learning there is no desired output associated with the data. Instead, we are interested in extracting some form of knowledge or model from the given data. In a sense, you can think of unsupervised learning as a means of discovering labels from the data itself. Unsupervised learning is often harder to understand and to evaluate. . Unsupervised learning comprises tasks such as dimensionality reduction, clustering, and density estimation. For example, in the iris data discussed above, we can use unsupervised methods to determine combinations of the measurements which best display the structure of the data. As we’ll see below, such a projection of the data can be used to visualize the four-dimensional dataset in two dimensions. Some more involved unsupervised learning problems are: . Given detailed observations of distant galaxies, determine which features or combinations of features summarize best the information. | Given a mixture of two sound sources (for example, a person talking over some music), separate the two (this is called the blind source separation problem). | Given a video, isolate a moving object and categorize in relation to other moving objects which have been seen. | Given a large collection of news articles, find recurring topics inside these articles. | Given a collection of images, cluster similar images together (for example to group them when visualizing a collection) | . Sometimes the two may even be combined: e.g. unsupervised learning can be used to find useful features in heterogeneous data, and then these features can be used within a supervised framework. . (simplified) Machine learning taxonomy . . Numpy Arrays . import numpy as np # Setting a random seed for reproducibility rnd = np.random.RandomState(seed=123) # Generating a random array X = rnd.uniform(low=0.0, high=1.0, size=(3, 5)) # a 3 x 5 array print(X) . [[0.69646919 0.28613933 0.22685145 0.55131477 0.71946897] [0.42310646 0.9807642 0.68482974 0.4809319 0.39211752] [0.34317802 0.72904971 0.43857224 0.0596779 0.39804426]] . (Note that NumPy arrays use 0-indexing just like other data structures in Python.) . # Accessing elements # get a single element # (here: an element in the first row and column) print(X[0, 0]) . 0.6964691855978616 . # get a row # (here: 2nd row) print(X[1]) . [0.42310646 0.9807642 0.68482974 0.4809319 0.39211752] . # get a column # (here: 2nd column) print(X[:, 1]) . [0.28613933 0.9807642 0.72904971] . type(X) . numpy.ndarray . # Transposing an array print(X.T) . [[0.69646919 0.42310646 0.34317802] [0.28613933 0.9807642 0.72904971] [0.22685145 0.68482974 0.43857224] [0.55131477 0.4809319 0.0596779 ] [0.71946897 0.39211752 0.39804426]] . begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 5 &amp; 6 &amp; 7 &amp; 8 end{bmatrix}= . begin{bmatrix} 1 &amp; 5 2 &amp; 6 3 &amp; 7 4 &amp; 8 end{bmatrix} # Creating a row vector # of evenly spaced numbers over a specified interval. y = np.linspace(0, 12, 5) print(y) . [ 0. 3. 6. 9. 12.] . # Turning the row vector into a column vector print(y[:, np.newaxis]) . [[ 0.] [ 3.] [ 6.] [ 9.] [12.]] . # Getting the shape or reshaping an array # Generating a random array rnd = np.random.RandomState(seed=123) X = rnd.uniform(low=0.0, high=1.0, size=(3, 5)) # a 3 x 5 array print(X.shape) print(X) . (3, 5) [[0.69646919 0.28613933 0.22685145 0.55131477 0.71946897] [0.42310646 0.9807642 0.68482974 0.4809319 0.39211752] [0.34317802 0.72904971 0.43857224 0.0596779 0.39804426]] . # Indexing by an array of integers (fancy indexing) indices = np.array([3, 1, 0]) . print(indices) X[:, indices] . [3 1 0] . array([[0.55131477, 0.28613933, 0.69646919], [0.4809319 , 0.9807642 , 0.42310646], [0.0596779 , 0.72904971, 0.34317802]]) . There is much, much more to know, but these few operations are fundamental to what we&#39;ll do during this tutorial. . SciPy Sparse Matrices . import scipy import numpy as np . from scipy import sparse # Create a random array with a lot of zeros rnd = np.random.RandomState(seed=123) X = rnd.uniform(low=0.0, high=1.0, size=(10, 5)) print(X) . [[0.69646919 0.28613933 0.22685145 0.55131477 0.71946897] [0.42310646 0.9807642 0.68482974 0.4809319 0.39211752] [0.34317802 0.72904971 0.43857224 0.0596779 0.39804426] [0.73799541 0.18249173 0.17545176 0.53155137 0.53182759] [0.63440096 0.84943179 0.72445532 0.61102351 0.72244338] [0.32295891 0.36178866 0.22826323 0.29371405 0.63097612] [0.09210494 0.43370117 0.43086276 0.4936851 0.42583029] [0.31226122 0.42635131 0.89338916 0.94416002 0.50183668] [0.62395295 0.1156184 0.31728548 0.41482621 0.86630916] [0.25045537 0.48303426 0.98555979 0.51948512 0.61289453]] . # set the majority of elements to zero X[X &lt; 0.7] = 0 print(X) . [[0. 0. 0. 0. 0.71946897] [0. 0.9807642 0. 0. 0. ] [0. 0.72904971 0. 0. 0. ] [0.73799541 0. 0. 0. 0. ] [0. 0.84943179 0.72445532 0. 0.72244338] [0. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. ] [0. 0. 0.89338916 0.94416002 0. ] [0. 0. 0. 0. 0.86630916] [0. 0. 0.98555979 0. 0. ]] . # turn X into a CSR (Compressed-Sparse-Row) matrix X_csr = sparse.csr_matrix(X) print(X_csr) type(X_csr) X_csr.shape . (0, 4) 0.7194689697855631 (1, 1) 0.9807641983846155 (2, 1) 0.7290497073840416 (3, 0) 0.7379954057320357 (4, 1) 0.8494317940777896 (4, 2) 0.7244553248606352 (4, 4) 0.7224433825702216 (7, 2) 0.8933891631171348 (7, 3) 0.9441600182038796 (8, 4) 0.8663091578833659 (9, 2) 0.985559785610705 . (10, 5) . # Converting the sparse matrix to a dense array print(X_csr.toarray()) . [[0. 0. 0. 0. 0.71946897] [0. 0.9807642 0. 0. 0. ] [0. 0.72904971 0. 0. 0. ] [0.73799541 0. 0. 0. 0. ] [0. 0.84943179 0.72445532 0. 0.72244338] [0. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. ] [0. 0. 0.89338916 0.94416002 0. ] [0. 0. 0. 0. 0.86630916] [0. 0. 0.98555979 0. 0. ]] . (You may have stumbled upon an alternative method for converting sparse to dense representations: numpy.todense; toarray returns a NumPy array, whereas todense returns a NumPy matrix. In this tutorial, we will be working with NumPy arrays, not matrices; the latter are not supported by scikit-learn.) . The CSR representation can be very efficient for computations, but it is not as good for adding elements. For that, the LIL (List-In-List) representation is better: . # Create an empty LIL matrix and add some items X_lil = sparse.lil_matrix((5, 5)) for i, j in np.random.randint(0, 5, (15, 2)): X_lil[i, j] = i + j print(X_lil) print(type(X_lil)) . (0, 1) 1.0 (0, 4) 4.0 (1, 2) 3.0 (1, 3) 4.0 (1, 4) 5.0 (2, 0) 2.0 (2, 3) 5.0 (3, 1) 4.0 (3, 3) 6.0 (3, 4) 7.0 (4, 1) 5.0 (4, 3) 7.0 &lt;class &#39;scipy.sparse.lil.lil_matrix&#39;&gt; . X_dense = X_lil.toarray() print(X_dense) print(type(X_dense)) . [[0. 1. 0. 0. 4.] [0. 0. 3. 4. 5.] [2. 0. 0. 5. 0.] [0. 4. 0. 6. 7.] [0. 5. 0. 7. 0.]] &lt;class &#39;numpy.ndarray&#39;&gt; . Often, once an LIL matrix is created, it is useful to convert it to a CSR format (many scikit-learn algorithms require CSR or CSC format) . X_csr = X_lil.tocsr() print(X_csr) print(type(X_csr)) . (0, 1) 1.0 (0, 4) 4.0 (1, 2) 3.0 (1, 3) 4.0 (1, 4) 5.0 (2, 0) 2.0 (2, 3) 5.0 (3, 1) 4.0 (3, 3) 6.0 (3, 4) 7.0 (4, 1) 5.0 (4, 3) 7.0 &lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt; . The available sparse formats that can be useful for various problems are: . CSR (compressed sparse row) | CSC (compressed sparse column) | BSR (block sparse row) | COO (coordinate) | DIA (diagonal) | DOK (dictionary of keys) | LIL (list in list) | . The scipy.sparse submodule also has a lot of functions for sparse matrices including linear algebra, sparse solvers, graph algorithms, and much more. . matplotlib . Another important part of machine learning is the visualization of data. The most common tool for this in Python is matplotlib. It is an extremely flexible package, and we will go over some basics here. . Since we are using Jupyter notebooks, let us use one of IPython&#39;s convenient built-in &quot;magic functions&quot;, the &quot;matoplotlib inline&quot; mode, which will draw the plots directly inside the notebook. . %matplotlib inline . import matplotlib.pyplot as plt . # Scatter-plot points x = np.random.normal(size=500) y = np.random.normal(size=500) plt.scatter(x, y); . # Showing images using imshow # - note that origin is at the top-left by default! x = np.linspace(1, 12, 100) y = x[:, np.newaxis] im = y * np.sin(x) * np.cos(y) print(im.shape) plt.imshow(im); . (100, 100) . # Contour plots # - note that origin here is at the bottom-left by default! plt.contour(im); . # 3D plotting from mpl_toolkits.mplot3d import Axes3D ax = plt.axes(projection=&#39;3d&#39;) xgrid, ygrid = np.meshgrid(x, y.ravel()) ax.plot_surface(xgrid, ygrid, im, cmap=plt.cm.viridis, cstride=2, rstride=2, linewidth=0); . There are many, many more plot types available. One useful way to explore these is by looking at the matplotlib gallery. . You can test these examples out easily in the notebook: simply copy the Source Code link on each page, and put it in a notebook using the %load magic. For example: . # %load http://matplotlib.org/mpl_examples/pylab_examples/ellipse_collection.py import matplotlib.pyplot as plt import numpy as np from matplotlib.collections import EllipseCollection x = np.arange(10) y = np.arange(15) X, Y = np.meshgrid(x, y) XY = np.hstack((X.ravel()[:, np.newaxis], Y.ravel()[:, np.newaxis])) ww = X/10.0 hh = Y/15.0 aa = X*9 fig, ax = plt.subplots() ec = EllipseCollection(ww, hh, aa, units=&#39;x&#39;, offsets=XY, transOffset=ax.transData) ec.set_array((X + Y).ravel()) ax.add_collection(ec) ax.autoscale_view() ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;y&#39;) cbar = plt.colorbar(ec) cbar.set_label(&#39;X+Y&#39;) plt.show() . import seaborn as sns . sns? . Type: module String form: &lt;module &#39;seaborn&#39; from &#39;C: Users babib anaconda3 envs mne lib site-packages seaborn __init__.py&#39;&gt; File: c: users babib anaconda3 envs mne lib site-packages seaborn __init__.py Docstring: &lt;no docstring&gt; . sns. .",
            "url": "https://berdakh.github.io/blog/eeg/jupyter/2020/10/18/ScikitLearn-tutorial-part1.html",
            "relUrl": "/eeg/jupyter/2020/10/18/ScikitLearn-tutorial-part1.html",
            "date": " • Oct 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Overview of artifact & noise detection",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;lcux-Fj-IYE&#39;, width=700, height=700) . %matplotlib inline . import os import numpy as np import mne sample_data_folder = mne.datasets.sample.data_path() sample_data_raw_file = os.path.join(sample_data_folder, &#39;MEG&#39;, &#39;sample&#39;, &#39;sample_audvis_raw.fif&#39;) raw = mne.io.read_raw_fif(sample_data_raw_file) raw.crop(0, 60).load_data() # just use a fraction of data for speed here . Opening raw data file C: Users babib mne_data MNE-sample-data MEG sample sample_audvis_raw.fif... Read a total of 3 projection items: PCA-v1 (1 x 102) idle PCA-v2 (1 x 102) idle PCA-v3 (1 x 102) idle Range : 25800 ... 192599 = 42.956 ... 320.670 secs Ready. Current compensation grade : 0 Reading 0 ... 36037 = 0.000 ... 60.000 secs... . &lt;Raw | sample_audvis_raw.fif, 376 x 36038 (60.0 s), ~107.0 MB, data loaded&gt; . What are artifacts? . Environmental artifacts . Persistent oscillations centered around the AC power line frequency_ (typically 50 or 60 Hz) | Brief signal jumps due to building vibration (such as a door slamming) | Electromagnetic field noise from nearby elevators, cell phones, the geomagnetic field, etc. | . | Instrumentation artifacts . Electromagnetic interference from stimulus presentation (such as EEG sensors picking up the field generated by unshielded headphones) | Continuous oscillations at specific frequencies used by head position indicator (HPI) coils | Random high-amplitude fluctuations (or alternatively, constant zero signal) in a single channel due to sensor malfunction (e.g., in surface electrodes, poor scalp contact) | . | Biological artifacts . Periodic QRS_-like signal patterns (especially in magnetometer channels) due to electrical activity of the heart | Short step-like deflections (especially in frontal EEG channels) due to eye movements | Large transient deflections (especially in frontal EEG channels) due to blinking | Brief bursts of high frequency fluctuations across several channels due to the muscular activity during swallowing | . | . Note . Artifacts of the same genesis may appear different in recordings made by different EEG or MEG systems, due to differences in sensor design (e.g., passive vs. active EEG electrodes; axial vs. planar gradiometers, etc). . What to do about artifacts . There are 3 basic options when faced with artifacts in your recordings: . Ignore the artifact and carry on with analysis | Exclude the corrupted portion of the data and analyze the remaining data | Repair the artifact by suppressing artifactual part of the recording while (hopefully) leaving the signal of interest intact | Artifact detection . MNE-Python includes a few tools for automated detection of certain artifacts (such as heartbeats and blinks), but of course you can always visually inspect your data to identify and annotate artifacts as well. . Low-frequency drifts . mag_channels = mne.pick_types(raw.info, meg=&#39;mag&#39;) raw.plot(duration=60, order=mag_channels, n_channels=len(mag_channels),remove_dc=False); . Power line noise . Power line artifacts are easiest to see on plots of the spectrum, so we&#39;ll use :meth:~mne.io.Raw.plot_psd to illustrate. . fig = raw.plot_psd(tmax=np.inf, fmax=250, average=True) # add some arrows at 60 Hz and its harmonics: for ax in fig.axes[:2]: freqs = ax.lines[-1].get_xdata() psds = ax.lines[-1].get_ydata() for freq in (60, 120, 180, 240): idx = np.searchsorted(freqs, freq) ax.arrow(x=freqs[idx], y=psds[idx] + 18, dx=0, dy=-12, color=&#39;red&#39;, width=0.1, head_width=3, length_includes_head=True) . Effective window size : 3.410 (s) Effective window size : 3.410 (s) Effective window size : 3.410 (s) . Heartbeat artifacts (ECG) . ecg_epochs = mne.preprocessing.create_ecg_epochs(raw) ecg_epochs.plot_image(combine=&#39;mean&#39;) . Reconstructing ECG signal from Magnetometers Setting up band-pass filter from 8 - 16 Hz FIR filter parameters Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter: - Windowed frequency-domain design (firwin2) method - Hann window - Lower passband edge: 8.00 - Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 7.75 Hz) - Upper passband edge: 16.00 Hz - Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 16.25 Hz) - Filter length: 8192 samples (13.639 sec) Number of ECG events detected : 59 (average pulse 58 / min.) 59 matching events found No baseline correction applied Not setting metadata Created an SSP operator (subspace dimension = 3) Loading data for 59 events and 601 original time points ... 0 bad epochs dropped 59 matching events found No baseline correction applied Not setting metadata 0 projection items activated 0 bad epochs dropped 59 matching events found No baseline correction applied Not setting metadata 0 projection items activated 0 bad epochs dropped 59 matching events found No baseline correction applied Not setting metadata 0 projection items activated 0 bad epochs dropped combining channels using &#34;mean&#34; combining channels using &#34;mean&#34; combining channels using &#34;mean&#34; . [&lt;Figure size 432x288 with 3 Axes&gt;, &lt;Figure size 432x288 with 3 Axes&gt;, &lt;Figure size 432x288 with 3 Axes&gt;] . The horizontal streaks in the magnetometer image plot reflect the fact that the heartbeat artifacts are superimposed on low-frequency drifts like the one we saw in an earlier section; to avoid this you could pass baseline=(-0.5, -0.2) in the call to :func:~mne.preprocessing.create_ecg_epochs. You can also get a quick look at the ECG-related field pattern across sensors by averaging the ECG epochs together via the :meth:~mne.Epochs.average method, and then using the :meth:mne.Evoked.plot_topomap method: . avg_ecg_epochs = ecg_epochs.average() . Here again we can visualize the spatial pattern of the associated field at various times relative to the peak of the EOG response: . avg_ecg_epochs.plot_topomap(times=np.linspace(-0.05, 0.05, 11)) . Or, we can get an ERP/F plot with :meth:~mne.Evoked.plot or a combined scalp field maps and ERP/F plot with :meth:~mne.Evoked.plot_joint. Here we&#39;ve specified the times for scalp field maps manually, but if not provided they will be chosen automatically based on peaks in the signal: . avg_ecg_epochs.plot_joint(times=[-0.25, -0.025, 0, 0.025, 0.25]); . Ocular artifacts (EOG) . eog_epochs = mne.preprocessing.create_eog_epochs(raw, baseline=(-0.5, -0.2)) eog_epochs.plot_image(combine=&#39;mean&#39;) eog_epochs.average().plot_joint() . EOG channel index for this subject is: [375] Filtering the data to remove DC offset to help distinguish blinks from saccades Setting up band-pass filter from 1 - 10 Hz FIR filter parameters Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter: - Windowed frequency-domain design (firwin2) method - Hann window - Lower passband edge: 1.00 - Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz) - Upper passband edge: 10.00 Hz - Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz) - Filter length: 8192 samples (13.639 sec) Now detecting blinks and generating corresponding events Found 10 significant peaks Number of EOG events detected : 10 10 matching events found Applying baseline correction (mode: mean) Not setting metadata Created an SSP operator (subspace dimension = 3) Loading data for 10 events and 601 original time points ... 0 bad epochs dropped 10 matching events found No baseline correction applied Not setting metadata 0 projection items activated 0 bad epochs dropped 10 matching events found No baseline correction applied Not setting metadata 0 projection items activated 0 bad epochs dropped 10 matching events found No baseline correction applied Not setting metadata 0 projection items activated 0 bad epochs dropped combining channels using &#34;mean&#34; combining channels using &#34;mean&#34; combining channels using &#34;mean&#34; . [&lt;Figure size 576x302.4 with 7 Axes&gt;, &lt;Figure size 576x302.4 with 7 Axes&gt;, &lt;Figure size 576x302.4 with 7 Axes&gt;] . Summary . Familiarizing yourself with typical artifact patterns and magnitudes is a crucial first step in assessing the efficacy of later attempts to repair those artifacts. A good rule of thumb is that the artifact amplitudes should be orders of magnitude larger than your signal of interest — and there should be several occurrences of such events — in order to find signal decompositions that effectively estimate and repair the artifacts. . Several other tutorials in this section illustrate the various tools for artifact repair, and discuss the pros and cons of each technique, for example: . tut-artifact-ssp | tut-artifact-ica | tut-artifact-sss | . There are also tutorials on general-purpose preprocessing steps such as filtering and resampling &lt;tut-filter-resample&gt; and excluding bad channels &lt;tut-bad-channels&gt; or `spans of data . `.&lt;/p&gt; .. LINKS . https://en.wikipedia.org/wiki/Mains_electricity . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; %matplotlib inline . Filtering and resampling data . import mne from mne.datasets import sample data_path = sample.data_path() raw_fname = data_path + &#39;/MEG/sample/sample_audvis_raw.fif&#39; . Setup for reading the raw data (save memory by cropping the raw data before loading it) . tmin, tmax = 0, 20 # use the first 20s of data raw = mne.io.read_raw_fif(raw_fname) raw.crop(tmin, tmax).load_data() raw.info[&#39;bads&#39;] = [&#39;MEG 2443&#39;, &#39;EEG 053&#39;] # bads + 2 more . Opening raw data file C: Users babib mne_data MNE-sample-data/MEG/sample/sample_audvis_raw.fif... Read a total of 3 projection items: PCA-v1 (1 x 102) idle PCA-v2 (1 x 102) idle PCA-v3 (1 x 102) idle Range : 25800 ... 192599 = 42.956 ... 320.670 secs Ready. Current compensation grade : 0 Reading 0 ... 12012 = 0.000 ... 20.000 secs... . Pick a subset of channels (here for speed reason) . selection = mne.read_selection(&#39;Left-temporal&#39;) picks = mne.pick_types(raw.info, meg=&#39;mag&#39;, eeg=False, eog=False, stim=False, exclude=&#39;bads&#39;, selection=selection) . Let&#39;s first check out all channel types . raw.plot_psd(area_mode=&#39;range&#39;, tmax=10.0, picks=picks, average=False); . Effective window size : 3.410 (s) . Removing power-line noise with notch filtering . Removing power-line noise can be done with a Notch filter, directly on the Raw object, specifying an array of frequency to be cut off: . import numpy as np raw.notch_filter(np.arange(60, 241, 60), picks=picks, filter_length=&#39;auto&#39;, phase=&#39;zero&#39;) raw.plot_psd(area_mode=&#39;range&#39;, tmax=10.0, picks=picks, average=False); . Setting up band-stop filter FIR filter parameters Designing a one-pass, zero-phase, non-causal bandstop filter: - Windowed time-domain design (firwin) method - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation - Lower transition bandwidth: 0.50 Hz - Upper transition bandwidth: 0.50 Hz - Filter length: 3965 samples (6.602 sec) Effective window size : 3.410 (s) . Removing power-line noise with low-pass filtering . If you&#39;re only interested in low frequencies, below the peaks of power-line noise you can simply low pass filter the data. . raw.filter(None, 50., fir_design=&#39;firwin&#39;) raw.plot_psd(area_mode=&#39;range&#39;, tmax=10.0, picks=picks, average=False); . Filtering raw data in 1 contiguous segment Setting up low-pass filter at 50 Hz FIR filter parameters Designing a one-pass, zero-phase, non-causal lowpass filter: - Windowed time-domain design (firwin) method - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation - Upper passband edge: 50.00 Hz - Upper transition bandwidth: 12.50 Hz (-6 dB cutoff frequency: 56.25 Hz) - Filter length: 159 samples (0.265 sec) Effective window size : 3.410 (s) . High-pass filtering to remove slow drifts . To remove slow drifts, you can high pass. . raw.filter(1., None, fir_design=&#39;firwin&#39;) raw.plot_psd(area_mode=&#39;range&#39;, tmax=10.0, picks=picks, average=False); . Filtering raw data in 1 contiguous segment Setting up high-pass filter at 1 Hz FIR filter parameters Designing a one-pass, zero-phase, non-causal highpass filter: - Windowed time-domain design (firwin) method - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation - Lower passband edge: 1.00 - Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz) - Filter length: 1983 samples (3.302 sec) Effective window size : 3.410 (s) . To do the low-pass and high-pass filtering in one step you can do a so-called band-pass filter by running the following: . raw.filter(1, 50., fir_design=&#39;firwin&#39;) . Filtering raw data in 1 contiguous segment Setting up band-pass filter from 1 - 50 Hz l_trans_bandwidth chosen to be 1.0 Hz h_trans_bandwidth chosen to be 12.5 Hz Filter length of 1983 samples (3.302 sec) selected . &lt;Raw | sample_audvis_raw.fif, n_channels x n_times : 376 x 12013 (20.0 sec), ~38.1 MB, data loaded&gt; . raw.plot(); . Downsampling and decimation . Data resampling can be done with resample methods. . raw.resample(100, npad=&quot;auto&quot;) # set sampling frequency to 100Hz raw.plot_psd(area_mode=&#39;range&#39;, tmax=10.0, picks=picks); . 25 events found Event IDs: [ 1 2 3 4 5 32] 25 events found Event IDs: [ 1 2 3 4 5 32] Effective window size : 10.010 (s) . raw.plot(); . Warning: This will reduce the timing precision of events . To avoid this reduction in precision, the suggested pipeline for processing final data to be analyzed is: . low-pass the data with mne.io.Raw.filter. | Extract epochs with mne.Epochs. | Decimate the Epochs object using mne.Epochs.decimate or the decim argument to the mne.Epochs object. | We also provide the convenience methods mne.Epochs.resample and mne.Evoked.resample to downsample or upsample data, but these are less optimal because they will introduce edge artifacts into every epoch, whereas filtering the raw data will only introduce edge artifacts only at the start and end of the recording. . Marking bad channels . If you already know which are the bad channels, simply do: . raw.info[&#39;bads&#39;] = [&#39;MEG 2443&#39;] . You can also mark them interactively in raw.plot(). The bad channel is shown in gray . ch_names = raw.info[&#39;ch_names&#39;].copy() ch_names.remove(&#39;MEG 2443&#39;) raw.reorder_channels([&#39;MEG 2443&#39;] + ch_names) raw.plot(); . It&#39;s not only raw. You can mark bads also in evoked. Let&#39;s first read in the evoked data. . # Reading data with a bad channel marked as bad: fname = data_path + &#39;/MEG/sample/sample_audvis-ave.fif&#39; evoked = mne.read_evokeds(fname, condition=&#39;Left Auditory&#39;, baseline=(None, 0)) evoked.pick_types(meg=&#39;grad&#39;, eeg=False, exclude=[]) # plot with bads evoked.plot(exclude=[], time_unit=&#39;s&#39;); . Reading C: Users babib mne_data MNE-sample-data/MEG/sample/sample_audvis-ave.fif ... Read a total of 4 projection items: PCA-v1 (1 x 102) active PCA-v2 (1 x 102) active PCA-v3 (1 x 102) active Average EEG reference (1 x 60) active Found the data of interest: t = -199.80 ... 499.49 ms (Left Auditory) 0 CTF compensation matrices available nave = 55 - aspect type = 100 Projections have already been applied. Setting proj attribute to True. Applying baseline correction (mode: mean) . It&#39;s also possible to repair the bad channels using interpolation . evoked.interpolate_bads(reset_bads=False, verbose=False); . Let’s plot the cleaned data . evoked.plot(exclude=[], time_unit=&#39;s&#39;); . Marking bad epochs . MNE allows you to specify rejection dictionary based on peak-to-peak thresholds for each channel type . reject = dict(grad=4000e-13, mag=4e-12, eog=200e-6) . events = mne.find_events(raw, stim_channel=&#39;STI 014&#39;) event_id = {&quot;auditory/left&quot;: 1} tmin, tmax = -0.2, 0.5 baseline = (None, 0) # means from the first instant to t = 0 picks = mne.pick_types(raw.info, meg=True, eeg=True, eog=True, stim=False, exclude=&#39;bads&#39;) epochs = mne.Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks, baseline=baseline, reject=reject, preload=True) . 25 events found Event IDs: [ 1 2 3 4 5 32] 6 matching events found Applying baseline correction (mode: mean) Not setting metadata Created an SSP operator (subspace dimension = 3) 3 projection items activated Loading data for 6 events and 71 original time points ... 0 bad epochs dropped . You can also reject after constructing epochs, just do: . reject.update({&#39;eog&#39;: 150e-6}) epochs.drop_bad(reject=reject) . 0 bad epochs dropped . &lt;Epochs | 6 events (all good), -0.2 - 0.5 sec, baseline [None, 0], ~4.8 MB, data loaded, &#39;auditory/left&#39;: 6&gt; . But the thresholds need to be stricter each time. . Tuning rejection thresholds . import matplotlib.pyplot as plt from ipywidgets import interact picks = mne.pick_types(raw.info, meg=False, eeg=True) epochs = mne.Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks, baseline=baseline, reject=None, preload=True) def reject_epochs(reject): reject = dict(eeg=reject * 1e-6) evoked = epochs.copy().drop_bad(reject=reject, verbose=False).average() evoked.plot(spatial_colors=True) print(&#39;Number of epochs retained: %d/%d&#39; % (evoked.nave, len(epochs))) . 6 matching events found Applying baseline correction (mode: mean) Not setting metadata 3 projection items activated Loading data for 6 events and 71 original time points ... 0 bad epochs dropped . interact(reject_epochs, reject=(35, 250, 10)); . Number of epochs retained: 6/6 . Autoreject . Autoreject (global) can compute the rejection dictionary automatically . http://autoreject.github.io/ . from autoreject import get_rejection_threshold # noqa reject = get_rejection_threshold(epochs) print(reject) . Estimating rejection dictionary for eeg {&#39;eeg&#39;: 5.2068012537220574e-05} . Autoreject (local) finds per channel thresholds: . . import numpy as np from autoreject import AutoReject n_interpolates = np.array([1, 2, 4]) consensus = np.linspace(0.5, 1.0, 6) ar = AutoReject(n_interpolates, consensus, thresh_method=&#39;random_search&#39;, random_state=42) . Now, we find the rejection thresholds per-channel and optimize for the number of channels to be interpolated. . raw.info[&#39;bads&#39;] = [] picks = mne.pick_types(raw.info, meg=&#39;grad&#39;, eeg=False) epochs = mne.Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=picks, baseline=baseline, reject=None, preload=True) # Note that fitting and transforming can be done on different compatible # portions of data if needed. ar.fit(epochs) . 6 matching events found Applying baseline correction (mode: mean) Not setting metadata 3 projection items activated Loading data for 6 events and 71 original time points ... 0 bad epochs dropped . Now, we can look at the rejection thresholds for each channel . for ch_name in epochs.info[&#39;ch_names&#39;][:5]: print(&#39;%s: %s&#39; % (ch_name, ar.threshes_[ch_name])) # plt.hist(np.array(list(ar.threshes_.values())), 30, color=&#39;g&#39;, alpha=0.4) . We can check what applying autoreject would do to the epochs: . Good data (light red) | Bad segment but not to be interpolated (medium dark red) | Bad segment to be interpolated (dark red) | . reject_log = ar.get_reject_log(epochs[&#39;auditory/left&#39;]) reject_log.plot() . Another way to visualize this is to plot them on the epochs . reject_log.plot_epochs(epochs[&#39;auditory/left&#39;]); . We can apply these rejection thresholds to new (or to the old) data: . epochs_clean = ar.transform(epochs[&#39;auditory/left&#39;]) . evoked = epochs[&#39;auditory/left&#39;].average() evoked.info[&#39;bads&#39;] = [&#39;MEG 2443&#39;] evoked.plot(exclude=[]); evoked_clean = epochs_clean.average() evoked_clean.info[&#39;bads&#39;] = [&#39;MEG 2443&#39;] evoked_clean.plot(exclude=[]); . For more info, visit . http://autoreject.github.io/ . &lt;/div&gt; .",
            "url": "https://berdakh.github.io/blog/eeg/mne/jupyter/2020/09/22/MNE-Overview-Of-Artifact.html",
            "relUrl": "/eeg/mne/jupyter/2020/09/22/MNE-Overview-Of-Artifact.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "MNE tutorial part 2",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;wNIaT1UT6rI&#39;, width=800, height=400) . %matplotlib inline import mne import matplotlib.pyplot as plt . fname = &quot;oddball-epo.fif&quot; . epochs = mne.read_epochs(fname) . Reading oddball-epo.fif ... Isotrak not found Found the data of interest: t = -200.00 ... 500.00 ms 0 CTF compensation matrices available 212 matching events found Applying baseline correction (mode: mean) Not setting metadata 0 projection items activated . epochs . &lt;EpochsFIF | 212 events (all good), -0.2 - 0.5 sec, baseline [-0.2, 0], ~7.5 MB, data loaded, &#39;standard/stimulus&#39;: 106 &#39;target/stimulus&#39;: 106&gt; . epochs.get_data().shape . (212, 64, 71) . Evoked . Finally, if we average an epoched dataset over trials, we can use the mne.Evoked object. . target = epochs[&quot;target&quot;].average() target . &lt;Evoked | &#39;target/stimulus&#39; (average, N=106), [-0.2, 0.5] sec, 63 ch, ~153 kB&gt; . standard = epochs[&quot;standard&quot;].average() . epochs[].get_data().shape . (212, 64, 71) . epochs[&#39;target&#39;].get_data().shape . (106, 64, 71) . target.data.shape . (63, 71) . target.info . &lt;Info | 10 non-empty values bads: [] ch_names: FP1, FP2, F7, F3, Fz, F4, F8, FC5, FC1, FC2, FC6, T7, C3, Cz, ... chs: 63 EEG custom_ref_applied: True file_id: 4 items (dict) highpass: 1.0 Hz lowpass: 20.0 Hz meas_date: 2017-08-10 20:05:16 UTC meas_id: 4 items (dict) nchan: 63 projs: [] sfreq: 100.0 Hz &gt; . To quickly investigate evoked activity, the Evoked object has a number of plotting functions available. . target.info[&#39;ch_names&#39;] . [&#39;FP1&#39;, &#39;FP2&#39;, &#39;F7&#39;, &#39;F3&#39;, &#39;Fz&#39;, &#39;F4&#39;, &#39;F8&#39;, &#39;FC5&#39;, &#39;FC1&#39;, &#39;FC2&#39;, &#39;FC6&#39;, &#39;T7&#39;, &#39;C3&#39;, &#39;Cz&#39;, &#39;C4&#39;, &#39;T8&#39;, &#39;CP5&#39;, &#39;CP1&#39;, &#39;CP2&#39;, &#39;CP6&#39;, &#39;P7&#39;, &#39;P3&#39;, &#39;Pz&#39;, &#39;P4&#39;, &#39;P8&#39;, &#39;PO9&#39;, &#39;O1&#39;, &#39;Oz&#39;, &#39;O2&#39;, &#39;PO10&#39;, &#39;AF7&#39;, &#39;AF3&#39;, &#39;AF4&#39;, &#39;AF8&#39;, &#39;F5&#39;, &#39;F1&#39;, &#39;F2&#39;, &#39;F6&#39;, &#39;SO1&#39;, &#39;FT7&#39;, &#39;FC3&#39;, &#39;FC4&#39;, &#39;FT8&#39;, &#39;SO2&#39;, &#39;C5&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C6&#39;, &#39;TP7&#39;, &#39;CP3&#39;, &#39;CPz&#39;, &#39;CP4&#39;, &#39;TP8&#39;, &#39;P5&#39;, &#39;P1&#39;, &#39;P2&#39;, &#39;P6&#39;, &#39;PO7&#39;, &#39;PO3&#39;, &#39;POz&#39;, &#39;PO4&#39;, &#39;PO8&#39;, &#39;FCz&#39;] . dir(target) . [&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__slotnames__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_aspect_kind&#39;, &#39;_data&#39;, &#39;_get_channel_positions&#39;, &#39;_pick_drop_channels&#39;, &#39;_projector&#39;, &#39;_set_channel_positions&#39;, &#39;_size&#39;, &#39;_update_first_last&#39;, &#39;add_channels&#39;, &#39;add_proj&#39;, &#39;animate_topomap&#39;, &#39;anonymize&#39;, &#39;apply_baseline&#39;, &#39;apply_hilbert&#39;, &#39;apply_proj&#39;, &#39;as_type&#39;, &#39;ch_names&#39;, &#39;comment&#39;, &#39;compensation_grade&#39;, &#39;copy&#39;, &#39;crop&#39;, &#39;data&#39;, &#39;decimate&#39;, &#39;del_proj&#39;, &#39;detrend&#39;, &#39;drop_channels&#39;, &#39;filter&#39;, &#39;first&#39;, &#39;get_channel_types&#39;, &#39;get_peak&#39;, &#39;info&#39;, &#39;interpolate_bads&#39;, &#39;kind&#39;, &#39;last&#39;, &#39;nave&#39;, &#39;pick&#39;, &#39;pick_channels&#39;, &#39;pick_types&#39;, &#39;picks&#39;, &#39;plot&#39;, &#39;plot_field&#39;, &#39;plot_image&#39;, &#39;plot_joint&#39;, &#39;plot_projs_topomap&#39;, &#39;plot_sensors&#39;, &#39;plot_topo&#39;, &#39;plot_topomap&#39;, &#39;plot_white&#39;, &#39;preload&#39;, &#39;proj&#39;, &#39;rename_channels&#39;, &#39;reorder_channels&#39;, &#39;resample&#39;, &#39;save&#39;, &#39;savgol_filter&#39;, &#39;set_channel_types&#39;, &#39;set_eeg_reference&#39;, &#39;set_meas_date&#39;, &#39;set_montage&#39;, &#39;shift_time&#39;, &#39;time_as_index&#39;, &#39;times&#39;, &#39;to_data_frame&#39;, &#39;verbose&#39;] . &#39;plot&#39;, &#39;plot_field&#39;, &#39;plot_image&#39;, &#39;plot_joint&#39;, &#39;plot_projs_topomap&#39;, &#39;plot_sensors&#39;, &#39;plot_topo&#39;, &#39;plot_topomap&#39;, &#39;plot_white&#39;, Visualization . target.plot(); . target.plot_topomap? . Signature: target.plot_topomap( times=&#39;auto&#39;, ch_type=None, layout=None, vmin=None, vmax=None, cmap=None, sensors=True, colorbar=True, scalings=None, units=None, res=64, size=1, cbar_fmt=&#39;%3.1f&#39;, time_unit=&#39;s&#39;, time_format=None, proj=False, show=True, show_names=False, title=None, mask=None, mask_params=None, outlines=&#39;head&#39;, contours=6, image_interp=&#39;bilinear&#39;, average=None, head_pos=None, axes=None, extrapolate=&#39;box&#39;, sphere=None, border=0, nrows=1, ncols=&#39;auto&#39;, ) Docstring: Plot topographic maps of specific time points of evoked data. Parameters - times : float | array of float | &#34;auto&#34; | &#34;peaks&#34; | &#34;interactive&#34; The time point(s) to plot. If &#34;auto&#34;, the number of ``axes`` determines the amount of time point(s). If ``axes`` is also None, at most 10 topographies will be shown with a regular time spacing between the first and last time instant. If &#34;peaks&#34;, finds time points automatically by checking for local maxima in global field power. If &#34;interactive&#34;, the time can be set interactively at run-time by using a slider. ch_type : &#39;mag&#39; | &#39;grad&#39; | &#39;planar1&#39; | &#39;planar2&#39; | &#39;eeg&#39; | None The channel type to plot. For &#39;grad&#39;, the gradiometers are collected in pairs and the RMS for each pair is plotted. If None, then channels are chosen in the order given above. layout : None Deprecated and will be removed in 0.21. Use ``sphere`` to control head-sensor relationship instead. vmin : float | callable | None The value specifying the lower bound of the color range. If None, and vmax is None, -vmax is used. Else np.min(data). If callable, the output equals vmin(data). Defaults to None. vmax : float | callable | None The value specifying the upper bound of the color range. If None, the maximum absolute value is used. If callable, the output equals vmax(data). Defaults to None. cmap : matplotlib colormap | (colormap, bool) | &#39;interactive&#39; | None Colormap to use. If tuple, the first value indicates the colormap to use and the second value is a boolean defining interactivity. In interactive mode the colors are adjustable by clicking and dragging the colorbar with left and right mouse button. Left mouse button moves the scale up and down and right mouse button adjusts the range (zoom). The mouse scroll can also be used to adjust the range. Hitting space bar resets the range. Up and down arrows can be used to change the colormap. If None (default), &#39;Reds&#39; is used for all positive data, otherwise defaults to &#39;RdBu_r&#39;. If &#39;interactive&#39;, translates to (None, True). .. warning:: Interactive mode works smoothly only for a small amount of topomaps. Interactive mode is disabled by default for more than 2 topomaps. sensors : bool | str Add markers for sensor locations to the plot. Accepts matplotlib plot format string (e.g., &#39;r+&#39; for red plusses). If True (default), circles will be used. colorbar : bool | None Plot a colorbar in the rightmost column of the figure. None (default) is the same as True, but emits a warning if custom ``axes`` are provided to remind the user that the colorbar will occupy the last :class:`matplotlib.axes.Axes` instance. scalings : dict | float | None The scalings of the channel types to be applied for plotting. If None, defaults to ``dict(eeg=1e6, grad=1e13, mag=1e15)``. units : dict | str | None The unit of the channel type used for colorbar label. If scale is None the unit is automatically determined. res : int The resolution of the topomap image (n pixels along each side). size : float Side length per topomap in inches. cbar_fmt : str String format for colorbar values. time_unit : str The units for the time axis, can be &#34;ms&#34; or &#34;s&#34; (default). .. versionadded:: 0.16 time_format : str | None String format for topomap values. Defaults (None) to &#34;%01d ms&#34; if ``time_unit=&#39;ms&#39;``, &#34;%0.3f s&#34; if ``time_unit=&#39;s&#39;``, and &#34;%g&#34; otherwise. proj : bool | &#39;interactive&#39; If true SSP projections are applied before display. If &#39;interactive&#39;, a check box for reversible selection of SSP projection vectors will be show. show : bool Show figure if True. show_names : bool | callable If True, show channel names on top of the map. If a callable is passed, channel names will be formatted using the callable; e.g., to delete the prefix &#39;MEG &#39; from all channel names, pass the function lambda x: x.replace(&#39;MEG &#39;, &#39;&#39;). If `mask` is not None, only significant sensors will be shown. title : str | None Title. If None (default), no title is displayed. mask : ndarray of bool, shape (n_channels, n_times) | None The channels to be marked as significant at a given time point. Indices set to `True` will be considered. Defaults to None. mask_params : dict | None Additional plotting parameters for plotting significant sensors. Default (None) equals:: dict(marker=&#39;o&#39;, markerfacecolor=&#39;w&#39;, markeredgecolor=&#39;k&#39;, linewidth=0, markersize=4) outlines : &#39;head&#39; | &#39;skirt&#39; | dict | None The outlines to be drawn. If &#39;head&#39;, the default head scheme will be drawn. If &#39;skirt&#39; the head scheme will be drawn, but sensors are allowed to be plotted outside of the head circle. If dict, each key refers to a tuple of x and y positions, the values in &#39;mask_pos&#39; will serve as image mask. Alternatively, a matplotlib patch object can be passed for advanced masking options, either directly or as a function that returns patches (required for multi-axis plots). If None, nothing will be drawn. Defaults to &#39;head&#39;. contours : int | array of float The number of contour lines to draw. If 0, no contours will be drawn. When an integer, matplotlib ticker locator is used to find suitable values for the contour thresholds (may sometimes be inaccurate, use array for accuracy). If an array, the values represent the levels for the contours. The values are in µV for EEG, fT for magnetometers and fT/m for gradiometers. If colorbar=True, the ticks in colorbar correspond to the contour levels. Defaults to 6. image_interp : str The image interpolation to be used. All matplotlib options are accepted. average : float | None The time window around a given time to be used for averaging (seconds). For example, 0.01 would translate into window that starts 5 ms before and ends 5 ms after a given time point. Defaults to None, which means no averaging. head_pos : dict | None Deprecated and will be removed in 0.21. Use ``sphere`` instead. axes : instance of Axes | list | None The axes to plot to. If list, the list must be a list of Axes of the same length as ``times`` (unless ``times`` is None). If instance of Axes, ``times`` must be a float or a list of one float. Defaults to None. extrapolate : str Options: - &#39;box&#39; (default) Extrapolate to four points placed to form a square encompassing all data points, where each side of the square is three times the range of the data in the respective dimension. - &#39;local&#39; Extrapolate only to nearby points (approximately to points closer than median inter-electrode distance). - &#39;head&#39; Extrapolate to the edges of the head circle (does not work well with sensors outside the head circle). .. versionadded:: 0.18 sphere : float | array-like | str | None The sphere parameters to use for the cartoon head. Can be array-like of shape (4,) to give the X/Y/Z origin and radius in meters, or a single float to give the radius (origin assumed 0, 0, 0). Can also be a spherical ConductorModel, which will use the origin and radius. Can be &#34;auto&#34; to use a digitization-based fit. Can also be None (default) to use &#39;auto&#39; when enough extra digitization points are available, and 0.095 otherwise. Currently the head radius does not affect plotting. .. versionadded:: 0.20 border : float | &#39;mean&#39; Value to extrapolate to on the topomap borders. If ``&#39;mean&#39;`` then each extrapolated point has the average value of its neighbours. .. versionadded:: 0.20 nrows : int | &#39;auto&#39; The number of rows of topographies to plot. Defaults to 1. If &#39;auto&#39;, obtains the number of rows depending on the amount of times to plot and the number of cols. Not valid when times == &#39;interactive&#39;. .. versionadded:: 0.20 ncols : int | &#39;auto&#39; The number of columns of topographies to plot. If &#39;auto&#39; (default), obtains the number of columns depending on the amount of times to plot and the number of rows. Not valid when times == &#39;interactive&#39;. .. versionadded:: 0.20 Returns - fig : instance of matplotlib.figure.Figure The figure. File: c: users babib anaconda3 envs mne lib site-packages mne evoked.py Type: method . Plot Topomap . target.plot_topomap(times = [0.1, 0.2, 0.3, 0.4, 0.5]); . target.plot_joint(times = [0.1, 0.2, 0.3, 0.37, 0.5]); . For condition contrasts, you can use mne.combine.evoked: . mne.combine_evoked? . Signature: mne.combine_evoked(all_evoked, weights) Docstring: Merge evoked data by weighted addition or subtraction. Data should have the same channels and the same time instants. Subtraction can be performed by calling ``combine_evoked([evoked1, -evoked2], &#39;equal&#39;)`` .. Warning:: If you provide an array of weights instead of using `&#39;equal&#39;` or `&#39;nave&#39;`, strange things may happen with your resulting signal amplitude and/or `.nave` attribute. Parameters - all_evoked : list of Evoked The evoked datasets. weights : list of float | str The weights to apply to the data of each evoked instance. Can also be ``&#39;nave&#39;`` to weight according to evoked.nave, or ``&#34;equal&#34;`` to use equal weighting (each weighted as ``1/N``). Returns - evoked : Evoked The new evoked data. Notes -- .. versionadded:: 0.9.0 File: c: users babib anaconda3 envs mne lib site-packages mne evoked.py Type: function . Plot Joint . diff = mne.combine_evoked((target, -standard), weights=&#39;equal&#39;) diff.plot_joint(times=[0.1, 0.2, 0.3, 0.37, 0.5]); . Or as an image: . diff.plot_image(); . Because we have a 10/20 electrode layout, we can easily use a somewhat nicer layout: . rois = mne.channels.make_1020_channel_selections(diff.info, midline=&quot;z12&quot;) . rois . {&#39;Left&#39;: array([25, 57, 58, 20, 53, 21, 48, 16, 49, 12, 44, 11, 40, 7, 39, 3, 34, 2, 31, 30]), &#39;Midline&#39;: array([27, 28, 26, 59, 54, 55, 22, 17, 18, 50, 46, 13, 45, 62, 9, 8, 4, 36, 35, 1, 0, 38, 43]), &#39;Right&#39;: array([29, 61, 60, 24, 56, 23, 52, 19, 51, 14, 15, 47, 41, 10, 42, 5, 37, 6, 32, 33])} . Plot Image by the Region of Interest . diff.plot_image(group_by=rois, show=False, show_names=&quot;all&quot;); . To contrast multiple conditions, mne.viz.plot_compare_evokeds is available: . mne.viz.plot_compare_evokeds({&quot;standard&quot;: standard, &quot;target&quot;: target}, picks=[9]); . &#39;plot&#39;, &#39;plot_field&#39;, &#39;plot_image&#39;, &#39;plot_joint&#39;, &#39;plot_projs_topomap&#39;, &#39;plot_sensors&#39;, &#39;plot_topo&#39;, &#39;plot_topomap&#39;, &#39;plot_white&#39;, target.plot_topo(); . target.plot_sensors(show_names = True); . target.data . array([[-2.10035784e-07, -2.76375979e-07, -4.35090469e-07, ..., -3.15511555e-06, -2.78982956e-06, -2.65591278e-06], [-1.14376608e-06, -1.12506920e-06, -1.19371126e-06, ..., -6.47561653e-06, -6.07672040e-06, -5.82352233e-06], [ 5.98786497e-07, 3.52630830e-07, 2.33978080e-07, ..., -9.69141043e-07, -8.53105041e-07, -9.22077181e-07], ..., [-4.99535795e-08, 3.00249622e-07, 3.81435930e-07, ..., 2.44255774e-06, 1.95231303e-06, 1.41322678e-06], [-3.46014338e-08, 1.47143539e-07, 2.53983361e-07, ..., 1.59443108e-06, 1.11796711e-06, 7.14063649e-07], [-1.44211725e-06, -1.33997168e-06, -1.55426496e-06, ..., -6.17147388e-06, -5.47561990e-06, -5.17723397e-06]]) . x = target.data . ch_names = target.info[&#39;ch_names&#39;] ch_names . [&#39;FP1&#39;, &#39;FP2&#39;, &#39;F7&#39;, &#39;F3&#39;, &#39;Fz&#39;, &#39;F4&#39;, &#39;F8&#39;, &#39;FC5&#39;, &#39;FC1&#39;, &#39;FC2&#39;, &#39;FC6&#39;, &#39;T7&#39;, &#39;C3&#39;, &#39;Cz&#39;, &#39;C4&#39;, &#39;T8&#39;, &#39;CP5&#39;, &#39;CP1&#39;, &#39;CP2&#39;, &#39;CP6&#39;, &#39;P7&#39;, &#39;P3&#39;, &#39;Pz&#39;, &#39;P4&#39;, &#39;P8&#39;, &#39;PO9&#39;, &#39;O1&#39;, &#39;Oz&#39;, &#39;O2&#39;, &#39;PO10&#39;, &#39;AF7&#39;, &#39;AF3&#39;, &#39;AF4&#39;, &#39;AF8&#39;, &#39;F5&#39;, &#39;F1&#39;, &#39;F2&#39;, &#39;F6&#39;, &#39;SO1&#39;, &#39;FT7&#39;, &#39;FC3&#39;, &#39;FC4&#39;, &#39;FT8&#39;, &#39;SO2&#39;, &#39;C5&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C6&#39;, &#39;TP7&#39;, &#39;CP3&#39;, &#39;CPz&#39;, &#39;CP4&#39;, &#39;TP8&#39;, &#39;P5&#39;, &#39;P1&#39;, &#39;P2&#39;, &#39;P6&#39;, &#39;PO7&#39;, &#39;PO3&#39;, &#39;POz&#39;, &#39;PO4&#39;, &#39;PO8&#39;, &#39;FCz&#39;] . channel = &#39;Cz&#39; chIndex = [i for i, j in enumerate(ch_names) if j == channel] . plt.plot(x.T) plt.title(channel) plt.ylabel(&quot;Amplitude&quot;) plt.xlabel(&#39;Time Samples&#39;) . Text(0.5, 0, &#39;Time Samples&#39;) . Time-Frequency stuff . For an overview over the spectral shape of the data, we can use a plotting method of raw, raw.plot_psd: . epochs_for_tfr = mne.read_epochs(&quot;oddball-long-epo.fif&quot;) . Reading oddball-long-epo.fif ... Isotrak not found Found the data of interest: t = -500.00 ... 1500.00 ms 0 CTF compensation matrices available 212 matching events found Applying baseline correction (mode: mean) Not setting metadata 0 projection items activated . plot_psd . epochs_for_tfr.plot_psd? . Signature: epochs_for_tfr.plot_psd( fmin=0, fmax=inf, tmin=None, tmax=None, proj=False, bandwidth=None, adaptive=False, low_bias=True, normalization=&#39;length&#39;, picks=None, ax=None, color=&#39;black&#39;, xscale=&#39;linear&#39;, area_mode=&#39;std&#39;, area_alpha=0.33, dB=True, estimate=&#39;auto&#39;, show=True, n_jobs=1, average=False, line_alpha=None, spatial_colors=True, sphere=None, verbose=None, ) Docstring: Plot the power spectral density across channels. Different channel types are drawn in sub-plots. When the data have been processed with a bandpass, lowpass or highpass filter, dashed lines indicate the boundaries of the filter (--). The line noise frequency is also indicated with a dashed line (-.). Parameters - fmin : float Start frequency to consider. fmax : float End frequency to consider. tmin : float | None Start time to consider. tmax : float | None End time to consider. proj : bool Apply projection. bandwidth : float The bandwidth of the multi taper windowing function in Hz. The default value is a window half-bandwidth of 4. adaptive : bool Use adaptive weights to combine the tapered spectra into PSD (slow, use n_jobs &gt;&gt; 1 to speed up computation). low_bias : bool Only use tapers with more than 90% spectral concentration within bandwidth. normalization : str Either &#34;full&#34; or &#34;length&#34; (default). If &#34;full&#34;, the PSD will be normalized by the sampling rate as well as the length of the signal (as in nitime). picks : str | list | slice | None Channels to include. Slices and lists of integers will be interpreted as channel indices. In lists, channel *type* strings (e.g., ``[&#39;meg&#39;, &#39;eeg&#39;]``) will pick channels of those types, channel *name* strings (e.g., ``[&#39;MEG0111&#39;, &#39;MEG2623&#39;]`` will pick the given channels. Can also be the string values &#34;all&#34; to pick all channels, or &#34;data&#34; to pick :term:`data channels`. None (default) will pick good data channels Cannot be None if `ax` is supplied.If both `picks` and `ax` are None separate subplots will be created for each standard channel type (`mag`, `grad`, and `eeg`). ax : instance of Axes | None Axes to plot into. If None, axes will be created. color : str | tuple A matplotlib-compatible color to use. Has no effect when spatial_colors=True. xscale : str Can be &#39;linear&#39; (default) or &#39;log&#39;. area_mode : str | None Mode for plotting area. If &#39;std&#39;, the mean +/- 1 STD (across channels) will be plotted. If &#39;range&#39;, the min and max (across channels) will be plotted. Bad channels will be excluded from these calculations. If None, no area will be plotted. If average=False, no area is plotted. area_alpha : float Alpha for the area. dB : bool Plot Power Spectral Density (PSD), in units (amplitude**2/Hz (dB)) if ``dB=True``, and ``estimate=&#39;power&#39;`` or ``estimate=&#39;auto&#39;``. Plot PSD in units (amplitude**2/Hz) if ``dB=False`` and, ``estimate=&#39;power&#39;``. Plot Amplitude Spectral Density (ASD), in units (amplitude/sqrt(Hz)), if ``dB=False`` and ``estimate=&#39;amplitude&#39;`` or ``estimate=&#39;auto&#39;``. Plot ASD, in units (amplitude/sqrt(Hz) (db)), if ``dB=True`` and ``estimate=&#39;amplitude&#39;``. estimate : str, {&#39;auto&#39;, &#39;power&#39;, &#39;amplitude&#39;} Can be &#34;power&#34; for power spectral density (PSD), &#34;amplitude&#34; for amplitude spectrum density (ASD), or &#34;auto&#34; (default), which uses &#34;power&#34; when dB is True and &#34;amplitude&#34; otherwise. show : bool Show figure if True. n_jobs : int The number of jobs to run in parallel (default 1). Requires the joblib package. average : bool If False, the PSDs of all channels is displayed. No averaging is done and parameters area_mode and area_alpha are ignored. When False, it is possible to paint an area (hold left mouse button and drag) to plot a topomap. line_alpha : float | None Alpha for the PSD line. Can be None (default) to use 1.0 when ``average=True`` and 0.1 when ``average=False``. spatial_colors : bool Whether to use spatial colors. Only used when ``average=False``. sphere : float | array-like | str | None The sphere parameters to use for the cartoon head. Can be array-like of shape (4,) to give the X/Y/Z origin and radius in meters, or a single float to give the radius (origin assumed 0, 0, 0). Can also be a spherical ConductorModel, which will use the origin and radius. Can be &#34;auto&#34; to use a digitization-based fit. Can also be None (default) to use &#39;auto&#39; when enough extra digitization points are available, and 0.095 otherwise. Currently the head radius does not affect plotting. .. versionadded:: 0.20 verbose : bool, str, int, or None If not None, override default verbose level (see :func:`mne.verbose` and :ref:`Logging documentation &lt;tut_logging&gt;` for more). Returns - fig : instance of Figure Figure with frequency spectra of the data channels. File: c: users babib anaconda3 envs mne lib site-packages mne epochs.py Type: method . epochs_for_tfr.info . &lt;Info | 10 non-empty values bads: [] ch_names: FP1, FP2, F7, F3, Fz, F4, F8, FC5, FC1, FC2, FC6, T7, C3, Cz, ... chs: 63 EEG, 1 STIM custom_ref_applied: True file_id: 4 items (dict) highpass: 1.0 Hz lowpass: 20.0 Hz meas_date: 2017-08-10 20:05:16 UTC meas_id: 4 items (dict) nchan: 64 projs: [] sfreq: 100.0 Hz &gt; . epochs_for_tfr.plot_psd(fmin=0, fmax=40); . Using multitaper spectrum estimation with 7 DPSS windows . But what about the time/frequency correlates of the Oddball effect? . We will extract power per time and frequency with Morlet wavelets. . from mne.time_frequency import tfr_morlet . mne.time_frequency.tfr_morlet? . Signature: mne.time_frequency.tfr_morlet( inst, freqs, n_cycles, use_fft=False, return_itc=True, decim=1, n_jobs=1, picks=None, zero_mean=True, average=True, output=&#39;power&#39;, verbose=None, ) Docstring: Compute Time-Frequency Representation (TFR) using Morlet wavelets. Parameters - inst : Epochs | Evoked The epochs or evoked object. freqs : ndarray, shape (n_freqs,) The frequencies in Hz. n_cycles : float | ndarray, shape (n_freqs,) The number of cycles globally or for each frequency. use_fft : bool, default False The fft based convolution or not. return_itc : bool, default True Return inter-trial coherence (ITC) as well as averaged power. Must be ``False`` for evoked data. decim : int | slice, default 1 To reduce memory usage, decimation factor after time-frequency decomposition. If `int`, returns tfr[..., ::decim]. If `slice`, returns tfr[..., decim]. .. note:: Decimation may create aliasing artifacts. n_jobs : int The number of jobs to run in parallel (default 1). Requires the joblib package. picks : array-like of int | None, default None The indices of the channels to decompose. If None, all available good data channels are decomposed. zero_mean : bool, default True Make sure the wavelet has a mean of zero. .. versionadded:: 0.13.0 average : bool, default True If True average across Epochs. .. versionadded:: 0.13.0 output : str Can be &#34;power&#34; (default) or &#34;complex&#34;. If &#34;complex&#34;, then average must be False. .. versionadded:: 0.15.0 verbose : bool, str, int, or None If not None, override default verbose level (see :func:`mne.verbose` and :ref:`Logging documentation &lt;tut_logging&gt;` for more). Returns - power : AverageTFR | EpochsTFR The averaged or single-trial power. itc : AverageTFR | EpochsTFR The inter-trial coherence (ITC). Only returned if return_itc is True. See Also -- mne.time_frequency.tfr_array_morlet mne.time_frequency.tfr_multitaper mne.time_frequency.tfr_array_multitaper mne.time_frequency.tfr_stockwell mne.time_frequency.tfr_array_stockwell File: c: users babib anaconda3 envs mne lib site-packages mne time_frequency tfr.py Type: function . freqs = list(range(3, 30)) tfr_target = tfr_morlet(epochs_for_tfr[&quot;target&quot;], freqs, 3, return_itc=False) tfr_standard = tfr_morlet(epochs_for_tfr[&quot;standard&quot;], freqs, 3, return_itc=False) . tfr_target.data.shape . (63, 27, 201) . Time-frequency data (single trial or averaged) is stored in TFR objects. These objects behave in many ways like Evoked objects ... . tfr_contrast = mne.combine_evoked((tfr_standard, tfr_target), (-.5, .5)) tfr_contrast.apply_baseline((None, 0)) . Applying baseline correction (mode: mean) . &lt;AverageTFR | time : [-0.500000, 1.500000], freq : [3.000000, 29.000000], nave : 212, channels : 63, ~2.7 MB&gt; . Plotting time-frequencyy activity (event-related spectral perturbations): observe the alpha-band ERD and the time-frequency correlates of the P3 effect. . tfr_contrast.plot_joint(); . No baseline correction applied No baseline correction applied . tfr_contrast.plot(picks=[13]); . No baseline correction applied .",
            "url": "https://berdakh.github.io/blog/eeg/jupyter/2020/09/14/MNE-Tutorial-part-2.html",
            "relUrl": "/eeg/jupyter/2020/09/14/MNE-Tutorial-part-2.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "MNE tutorial part 1",
            "content": "from IPython.display import YouTubeVideo YouTubeVideo(&#39;IYuAPisoUeI&#39;, width=800, height=400) . %matplotlib inline import mne import matplotlib.pyplot as plt . fname = &quot;oddball_example_small-fif.gz&quot; . Read in raw data; raw objects . raw = mne.io.read_raw_fif(fname) . Opening raw data file oddball_example_small-fif.gz... . &lt;ipython-input-4-78767f98f250&gt;:1: RuntimeWarning: This filename (oddball_example_small-fif.gz) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz or _meg.fif raw = mne.io.read_raw_fif(fname) . Isotrak not found Range : 2903 ... 112000 = 29.030 ... 1120.000 secs Ready. . Visualize sample data . raw.plot(duration=60.0, start=0.0, n_channels=16,); . The information about the object can be found at raw.info . print(raw.info) . &lt;Info | 10 non-empty values bads: [] ch_names: FP1, FP2, F7, F3, Fz, F4, F8, FC5, FC1, FC2, FC6, T7, C3, Cz, ... chs: 63 EEG, 1 STIM custom_ref_applied: True file_id: 4 items (dict) highpass: 0.1 Hz lowpass: 30.0 Hz meas_date: 2017-08-10 20:05:16 UTC meas_id: 4 items (dict) nchan: 64 projs: [] sfreq: 100.0 Hz &gt; . MNE is object oriented. Objects have corresponding methods. Check which by typing raw. and pressing TAB: . raw.filter . &lt;bound method BaseRaw.filter of &lt;Raw | oddball_example_small-fif.gz, 64 x 109098 (1091.0 s), ~120 kB, data not loaded&gt;&gt; . raw.resample raw.filter raw.drop_channels ... . Can we do further preprocessing?.. . raw.filter(1, 20) . RuntimeError Traceback (most recent call last) &lt;ipython-input-8-780629dd327e&gt; in &lt;module&gt; -&gt; 1 raw.filter(1, 20) ~ anaconda3 envs mne lib site-packages mne io base.py in filter(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose) 926 skip_by_annotation=(&#39;edge&#39;, &#39;bad_acq_skip&#39;), 927 pad=&#39;reflect_limited&#39;, verbose=None): # noqa: D102 --&gt; 928 return super().filter( 929 l_freq, h_freq, picks, filter_length, l_trans_bandwidth, 930 h_trans_bandwidth, n_jobs, method, iir_params, phase, &lt;decorator-gen-111&gt; in filter(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose) ~ anaconda3 envs mne lib site-packages mne filter.py in filter(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose) 1920 &#34;&#34;&#34; 1921 from .io.base import BaseRaw -&gt; 1922 _check_preload(self, &#39;inst.filter&#39;) 1923 if pad is None and method != &#39;iir&#39;: 1924 pad = &#39;edge&#39; ~ anaconda3 envs mne lib site-packages mne utils check.py in _check_preload(inst, msg) 187 name = &#34;epochs&#34; if isinstance(inst, BaseEpochs) else &#39;raw&#39; 188 if not inst.preload: --&gt; 189 raise RuntimeError( 190 &#34;By default, MNE does not load data into main memory to &#34; 191 &#34;conserve resources. &#34; + msg + &#39; requires %s data to be &#39; RuntimeError: By default, MNE does not load data into main memory to conserve resources. inst.filter requires raw data to be loaded. Use preload=True (or string) in the constructor or raw.load_data(). . By default, MNE does not store raw and epochs objects in memory. . raw = mne.io.read_raw_fif(fname, preload=True) . Opening raw data file oddball_example_small-fif.gz... . &lt;ipython-input-9-7341ee706cdd&gt;:1: RuntimeWarning: This filename (oddball_example_small-fif.gz) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz or _meg.fif raw = mne.io.read_raw_fif(fname, preload=True) . Isotrak not found Range : 2903 ... 112000 = 29.030 ... 1120.000 secs Ready. Reading 0 ... 109097 = 0.000 ... 1090.970 secs... . raw.filter(1, 20) . Filtering raw data in 1 contiguous segment Setting up band-pass filter from 1 - 20 Hz FIR filter parameters Designing a one-pass, zero-phase, non-causal bandpass filter: - Windowed time-domain design (firwin) method - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation - Lower passband edge: 1.00 - Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz) - Upper passband edge: 20.00 Hz - Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz) - Filter length: 331 samples (3.310 sec) . &lt;Raw | oddball_example_small-fif.gz, 64 x 109098 (1091.0 s), ~53.4 MB, data loaded&gt; . Inspecting raw data ... . raw.plot(duration=60.0, start=0.0, n_channels=16,); . There are many eog artefacts. We will use ICA to correct these. For this, we create an ICA object and use its .fit method on a filtered copy of the raw data: . ICA decomposition . ica = mne.preprocessing.ICA(n_components=20, random_state=0) . ica.fit(raw.copy().filter(8, 35)) . Filtering raw data in 1 contiguous segment Setting up band-pass filter from 8 - 35 Hz FIR filter parameters Designing a one-pass, zero-phase, non-causal bandpass filter: - Windowed time-domain design (firwin) method - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation - Lower passband edge: 8.00 - Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz) - Upper passband edge: 35.00 Hz - Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz) - Filter length: 165 samples (1.650 sec) Fitting ICA to data using 63 channels (please be patient, this may take a while) Inferring max_pca_components from picks Selecting by number: 20 components Fitting ICA took 6.9s. . &lt;ICA | raw data decomposition, fit (fastica): 109098 samples, 20 components, channels used: &#34;eeg&#34;&gt; . ica.plot_components(outlines=&quot;skirt&quot;); . We store &quot;bad&quot; components in the ica object. . ica.exclude = [1, 10, 14, 17, 18, 19] . We could also use one of the automatic algorithms ... . bad_idx, scores = ica.find_bads_eog(raw, &#39;SO2&#39;, threshold=2) print(bad_idx) . Using channel SO2 as EOG channel ... filtering ICA sources Setting up band-pass filter from 1 - 10 Hz FIR filter parameters Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter: - Windowed frequency-domain design (firwin2) method - Hann window - Lower passband edge: 1.00 - Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz) - Upper passband edge: 10.00 Hz - Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz) - Filter length: 1024 samples (10.240 sec) ... filtering target Setting up band-pass filter from 1 - 10 Hz FIR filter parameters Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter: - Windowed frequency-domain design (firwin2) method - Hann window - Lower passband edge: 1.00 - Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz) - Upper passband edge: 10.00 Hz - Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz) - Filter length: 1024 samples (10.240 sec) [14, 10] . Let&#39;s compare raw and corrected data ... . Before ICA plot . raw.plot(duration=60.0, start=0.0, n_channels=16,); . After ICA plot . ica.apply(raw.copy(), exclude=ica.exclude).plot(duration=60.0, start=0.0, n_channels=16); . Transforming to ICA space (20 components) Zeroing out 6 ICA components . Epochs . For epoching the data, we need event markers. Usually, these are stored in the raw object; in MNE, in a stimulus channel. . mne.find_events? . Signature: mne.find_events( raw, stim_channel=None, output=&#39;onset&#39;, consecutive=&#39;increasing&#39;, min_duration=0, shortest_event=2, mask=None, uint_cast=False, mask_type=&#39;and&#39;, initial_event=False, verbose=None, ) Docstring: Find events from raw file. See :ref:`tut-events-vs-annotations` and :ref:`tut-event-arrays` for more information about events. Parameters - raw : Raw object The raw data. stim_channel : None | str | list of str Name of the stim channel or all the stim channels affected by triggers. If None, the config variables &#39;MNE_STIM_CHANNEL&#39;, &#39;MNE_STIM_CHANNEL_1&#39;, &#39;MNE_STIM_CHANNEL_2&#39;, etc. are read. If these are not found, it will fall back to &#39;STI 014&#39; if present, then fall back to the first channel of type &#39;stim&#39;, if present. If multiple channels are provided then the returned events are the union of all the events extracted from individual stim channels. output : &#39;onset&#39; | &#39;offset&#39; | &#39;step&#39; Whether to report when events start, when events end, or both. consecutive : bool | &#39;increasing&#39; If True, consider instances where the value of the events channel changes without first returning to zero as multiple events. If False, report only instances where the value of the events channel changes from/to zero. If &#39;increasing&#39;, report adjacent events only when the second event code is greater than the first. min_duration : float The minimum duration of a change in the events channel required to consider it as an event (in seconds). shortest_event : int Minimum number of samples an event must last (default is 2). If the duration is less than this an exception will be raised. mask : int | None The value of the digital mask to apply to the stim channel values. If None (default), no masking is performed. uint_cast : bool If True (default False), do a cast to ``uint16`` on the channel data. This can be used to fix a bug with STI101 and STI014 in Neuromag acquisition setups that use channel STI016 (channel 16 turns data into e.g. -32768), similar to ``mne_fix_stim14 --32`` in MNE-C. .. versionadded:: 0.12 mask_type : &#39;and&#39; | &#39;not_and&#39; The type of operation between the mask and the trigger. Choose &#39;and&#39; (default) for MNE-C masking behavior. .. versionadded:: 0.13 initial_event : bool If True (default False), an event is created if the stim channel has a value different from 0 as its first sample. This is useful if an event at t=0s is present. .. versionadded:: 0.16 verbose : bool, str, int, or None If not None, override default verbose level (see :func:`mne.verbose` and :ref:`Logging documentation &lt;tut_logging&gt;` for more). Returns - events : array, shape = (n_events, 3) All events that were found. The first column contains the event time in samples and the third column contains the event id. For output = &#39;onset&#39; or &#39;step&#39;, the second column contains the value of the stim channel immediately before the event/step. For output = &#39;offset&#39;, the second column contains the value of the stim channel after the event offset. See Also -- find_stim_steps : Find all the steps in the stim channel. read_events : Read events from disk. write_events : Write events to disk. Notes -- .. warning:: If you are working with downsampled data, events computed before decimation are no longer valid. Please recompute your events after decimation, but note this reduces the precision of event timing. Examples -- Consider data with a stim channel that looks like:: [0, 32, 32, 33, 32, 0] By default, find_events returns all samples at which the value of the stim channel increases:: &gt;&gt;&gt; print(find_events(raw)) # doctest: +SKIP [[ 1 0 32] [ 3 32 33]] If consecutive is False, find_events only returns the samples at which the stim channel changes from zero to a non-zero value:: &gt;&gt;&gt; print(find_events(raw, consecutive=False)) # doctest: +SKIP [[ 1 0 32]] If consecutive is True, find_events returns samples at which the event changes, regardless of whether it first returns to zero:: &gt;&gt;&gt; print(find_events(raw, consecutive=True)) # doctest: +SKIP [[ 1 0 32] [ 3 32 33] [ 4 33 32]] If output is &#39;offset&#39;, find_events returns the last sample of each event instead of the first one:: &gt;&gt;&gt; print(find_events(raw, consecutive=True, # doctest: +SKIP ... output=&#39;offset&#39;)) [[ 2 33 32] [ 3 32 33] [ 4 0 32]] If output is &#39;step&#39;, find_events returns the samples at which an event starts or ends:: &gt;&gt;&gt; print(find_events(raw, consecutive=True, # doctest: +SKIP ... output=&#39;step&#39;)) [[ 1 0 32] [ 3 32 33] [ 4 33 32] [ 5 32 0]] To ignore spurious events, it is also possible to specify a minimum event duration. Assuming our events channel has a sample rate of 1000 Hz:: &gt;&gt;&gt; print(find_events(raw, consecutive=True, # doctest: +SKIP ... min_duration=0.002)) [[ 1 0 32]] For the digital mask, if mask_type is set to &#39;and&#39; it will take the binary representation of the digital mask, e.g. 5 -&gt; &#39;00000101&#39;, and will allow the values to pass where mask is one, e.g.:: 7 &#39;0000111&#39; &lt;- trigger value 37 &#39;0100101&#39; &lt;- mask - 5 &#39;0000101&#39; For the digital mask, if mask_type is set to &#39;not_and&#39; it will take the binary representation of the digital mask, e.g. 5 -&gt; &#39;00000101&#39;, and will block the values where mask is one, e.g.:: 7 &#39;0000111&#39; &lt;- trigger value 37 &#39;0100101&#39; &lt;- mask - 2 &#39;0000010&#39; File: c: users babib anaconda3 envs mne lib site-packages mne event.py Type: function . events = mne.find_events(raw) . 903 events found Event IDs: [100 200] . events is simply an array (time in samples, zero, trigger); . events . array([[ 3241, 0, 200], [ 3437, 0, 200], [ 3643, 0, 200], ..., [111496, 0, 200], [111613, 0, 200], [111719, 0, 200]], dtype=int64) . ... which we can visualize: . plt.rcParams[&#39;figure.figsize&#39;] = [10, 5] . mne.viz.plot_events(events[:100]); . For creating an mne.Epochs object, we require, in addition to the raw object and the events array, a dictionary of the intended condition names and the corresponding trigger numbers. . event_ids = {&quot;standard/stimulus&quot;: 200, &quot;target/stimulus&quot;: 100} epochs = mne.Epochs(raw, events, event_id=event_ids) . 903 matching events found Applying baseline correction (mode: mean) Not setting metadata 0 projection items activated . epochs.plot(); . Loading data for 903 events and 71 original time points ... 0 bad epochs dropped Loading data for 903 events and 71 original time points ... Loading data for 20 events and 71 original time points ... . (changing to the inline backend now to speed things up.) . epochs = ica.apply(epochs, exclude=ica.exclude) . RuntimeError Traceback (most recent call last) &lt;ipython-input-26-6221a76f7f3f&gt; in &lt;module&gt; -&gt; 1 epochs = ica.apply(epochs, exclude=ica.exclude) ~ anaconda3 envs mne lib site-packages mne preprocessing ica.py in apply(self, inst, include, exclude, n_pca_components, start, stop) 1405 _check_compensation_grade(self.info, inst.info, &#39;ICA&#39;, kind, 1406 ch_names=self.ch_names) -&gt; 1407 return meth(**kwargs) 1408 1409 def _check_exclude(self, exclude): ~ anaconda3 envs mne lib site-packages mne preprocessing ica.py in _apply_epochs(self, epochs, include, exclude, n_pca_components) 1436 def _apply_epochs(self, epochs, include, exclude, n_pca_components): 1437 &#34;&#34;&#34;Aux method.&#34;&#34;&#34; -&gt; 1438 _check_preload(epochs, &#34;ica.apply&#34;) 1439 1440 picks = pick_types(epochs.info, meg=False, ref_meg=False, ~ anaconda3 envs mne lib site-packages mne utils check.py in _check_preload(inst, msg) 187 name = &#34;epochs&#34; if isinstance(inst, BaseEpochs) else &#39;raw&#39; 188 if not inst.preload: --&gt; 189 raise RuntimeError( 190 &#34;By default, MNE does not load data into main memory to &#34; 191 &#34;conserve resources. &#34; + msg + &#39; requires %s data to be &#39; RuntimeError: By default, MNE does not load data into main memory to conserve resources. ica.apply requires epochs data to be loaded. Use preload=True (or string) in the constructor or epochs.load_data(). . Of course ... . epochs = mne.Epochs(raw, events, event_id=event_ids, preload=True) epochs = ica.apply(epochs, exclude=ica.exclude) . 903 matching events found Applying baseline correction (mode: mean) Not setting metadata 0 projection items activated Loading data for 903 events and 71 original time points ... 0 bad epochs dropped Transforming to ICA space (20 components) Zeroing out 6 ICA components . The mne.Epochs constructor has a number of options, such as time window lengths and rejection thresholds. Investigate them on your own. . Epochs objects also have various methods, different from raw objects - e.g., for baselining. . epochs.apply_baseline((None, 0)) . Applying baseline correction (mode: mean) . &lt;Epochs | 903 events (all good), -0.2 - 0.5 sec, baseline [None, 0], ~31.4 MB, data loaded, &#39;standard/stimulus&#39;: 797 &#39;target/stimulus&#39;: 106&gt; . ... and many more ... . epochs. . File &#34;&lt;ipython-input-29-0e32a7ceced3&gt;&#34;, line 1 epochs. ^ SyntaxError: invalid syntax . To subselect only a sample of epochs, a dict-like access mode is available. . epochs . &lt;Epochs | 903 events (all good), -0.2 - 0.5 sec, baseline [None, 0], ~31.4 MB, data loaded, &#39;standard/stimulus&#39;: 797 &#39;target/stimulus&#39;: 106&gt; . epochs[&quot;target&quot;] . &lt;Epochs | 106 events (all good), -0.2 - 0.5 sec, baseline [None, 0], ~3.8 MB, data loaded, &#39;target/stimulus&#39;: 106&gt; . Observe how tags selected by forward slashes - &quot;/&quot; - work. . epochs[&quot;stimulus&quot;] . &lt;Epochs | 903 events (all good), -0.2 - 0.5 sec, baseline [None, 0], ~31.4 MB, data loaded, &#39;standard/stimulus&#39;: 797 &#39;target/stimulus&#39;: 106&gt; . How does the epoched activity look like? . epochs.info . &lt;Info | 10 non-empty values bads: [] ch_names: FP1, FP2, F7, F3, Fz, F4, F8, FC5, FC1, FC2, FC6, T7, C3, Cz, ... chs: 63 EEG, 1 STIM custom_ref_applied: True file_id: 4 items (dict) highpass: 1.0 Hz lowpass: 20.0 Hz meas_date: 2017-08-10 20:05:16 UTC meas_id: 4 items (dict) nchan: 64 projs: [] sfreq: 100.0 Hz &gt; . epochs[&quot;target&quot;].plot_image(picks=[13]); . 106 matching events found No baseline correction applied Not setting metadata 0 projection items activated 0 bad epochs dropped . To ensure we have as many Oddball as Standard trials, we can run ... . epochs.equalize_event_counts(event_ids) epochs . Dropped 691 epochs: 0, 1, 2, 3, 4, 5, 6, 9, 12, 13, 14, 15, 18, 19, 20, 21, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 107, 108, 109, 110, 113, 114, 115, 116, 117, 118, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 141, 142, 143, 146, 147, 148, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160, 161, 162, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 187, 188, 193, 194, 197, 198, 199, 200, 201, 202, 203, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 248, 249, 252, 253, 254, 257, 260, 261, 262, 263, 266, 267, 268, 269, 270, 271, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 290, 291, 292, 295, 296, 297, 298, 299, 300, 301, 302, 303, 306, 307, 308, 309, 310, 311, 312, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 343, 344, 345, 346, 347, 348, 349, 350, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 368, 369, 370, 371, 372, 375, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 432, 433, 434, 435, 438, 439, 442, 449, 450, 451, 452, 453, 458, 459, 464, 465, 466, 467, 472, 473, 474, 475, 476, 477, 482, 483, 484, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 546, 547, 548, 551, 552, 553, 554, 555, 558, 559, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 610, 611, 612, 615, 616, 619, 620, 621, 624, 625, 626, 627, 628, 629, 630, 631, 632, 635, 636, 637, 638, 641, 642, 643, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 674, 675, 676, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 780, 781, 782, 785, 786, 787, 788, 791, 792, 793, 794, 795, 796, 799, 802, 805, 806, 809, 810, 811, 812, 813, 814, 817, 818, 821, 822, 823, 826, 827, 828, 829, 830, 831, 832, 833, 834, 837, 838, 839, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 872, 875, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902 . &lt;Epochs | 212 events (all good), -0.2 - 0.5 sec, baseline [None, 0], ~7.5 MB, data loaded, &#39;standard/stimulus&#39;: 106 &#39;target/stimulus&#39;: 106&gt; . We can write the Epochs object to disk so we don&#39;t have to repeat the preprocessing later ... . Save data . epochs.save(&quot;oddball2-epo.fif&quot;) # remember, the data has been cleaned of bad ICs . epochs_for_tfr = mne.Epochs(raw, events, event_id=event_ids, tmin=-.5, tmax=1.5, preload=True) # need longer data segment . 903 matching events found Applying baseline correction (mode: mean) Not setting metadata 0 projection items activated Loading data for 903 events and 201 original time points ... 0 bad epochs dropped . epochs_for_tfr.plot(); . epochs_for_tfr = ica.apply(epochs_for_tfr, exclude=ica.exclude) epochs_for_tfr.equalize_event_counts(event_ids); # to speed up things #epochs_for_tfr.save(&quot;oddball-long-epo.fif&quot;) . Transforming to ICA space (20 components) Zeroing out 6 ICA components Dropped 0 epochs: . Get Numpy Array . X = epochs.get_data() . X.shape . (212, 64, 71) . type(X) . numpy.ndarray . epochs[&#39;target&#39;].get_data().shape . (106, 64, 71) . Xtarget = epochs[&#39;target&#39;].get_data() Xnontarget = epochs[&#39;standard&#39;].get_data() . Xtarget.shape . (106, 64, 71) . Xnontarget.shape . (106, 64, 71) .",
            "url": "https://berdakh.github.io/blog/eeg/jupyter/2020/09/10/MNE-Tutorial.html",
            "relUrl": "/eeg/jupyter/2020/09/10/MNE-Tutorial.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Motor Imagery EEG visualization example",
            "content": "import matplotlib.pyplot as plt import numpy as np . %matplotlib inline %load_ext autoreload %autoreload 2 #%matplotlib qt . plt.rcParams[&#39;font.size&#39;] = 12 #plt.style.use(&#39;ggplot&#39;) plt.rcParams[&quot;axes.grid&quot;] = True c = plt.rcParams[&#39;axes.prop_cycle&#39;].by_key()[&#39;color&#39;] plt.rcParams[&#39;figure.figsize&#39;] = 8, 4 . from nu_smrutils import loaddat . dname = dict(BNCI2014004 = &#39;aBNCI2014004R.pickle&#39;, BNCI2014001 = &#39;aBNCI2014001R.pickle&#39;, Weibo2014 = &#39;aWeibo2014R.pickle&#39;, Physionet = &#39;aPhysionetRR.pickle&#39;) . Load data . Load EEG data for visualization . # itemname is one of : [&#39;BNCI2014004&#39;, &#39;BNCI2014001&#39;, &#39;Weibo2014&#39;, &#39;Physionet&#39;] itemname = &#39;BNCI2014001&#39; . filename = dname[itemname] iname = itemname + &#39;__&#39; data = loaddat(filename) print(&#39;Number of subjects in data :&#39;, len(data)) . Number of subjects in data : 9 . # select data from on subject and use it for demostration subject = 0 s1 = data[subject] print(s1) . &lt;Epochs | 288 events (all good), 2 - 6 sec, baseline off, ~15.6 MB, data loaded, &#39;left_hand&#39;: 144 &#39;right_hand&#39;: 144&gt; . s1[&#39;right_hand&#39;].plot(n_epochs=10); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:33:48.954226 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ s1[&#39;right_hand&#39;].plot_psd(fmin=0, fmax=40); . Using multitaper spectrum estimation with 7 DPSS windows . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:30:09.389797 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Apply common-average reference . # use the average of all channels as reference s1.set_eeg_reference(ref_channels=&#39;average&#39;) s1[&#39;right_hand&#39;].plot(n_epochs=10); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:33:43.620625 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ s1[&#39;right_hand&#39;].plot_psd(fmin=0, fmax=40); . Using multitaper spectrum estimation with 7 DPSS windows . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:30:26.847473 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Data info . print(s1.info) . &lt;Info | 9 non-empty values bads: [] ch_names: Fz, FC3, FC1, FCz, FC2, FC4, C5, C3, C1, Cz, C2, C4, C6, CP3, ... chs: 22 EEG custom_ref_applied: True dig: 25 items (3 Cardinal, 22 EEG) highpass: 4.0 Hz lowpass: 60.0 Hz meas_date: unspecified nchan: 22 projs: [] sfreq: 80.0 Hz &gt; . s1.plot_sensors(title = &#39;EEG sensor locations and labels&#39;, show_names = True); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:30:33.251171 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Band-pass filter . Apply band-pass filter to extract $ mu$ and $ beta$ band EEG features between (8 - 30) Hz . s1.filter(l_freq = 8, h_freq = 30) . &lt;Epochs | 288 events (all good), 2 - 6 sec, baseline off, ~15.6 MB, data loaded, &#39;left_hand&#39;: 144 &#39;right_hand&#39;: 144&gt; . s1[&#39;right_hand&#39;].plot(n_epochs=10); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:34:01.941700 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ s1[&#39;right_hand&#39;].plot_image(picks = [&#39;Cz&#39;], scalings = dict(eeg=1e6)) . Not setting metadata Not setting metadata 144 matching events found No baseline correction applied 0 projection items activated 0 bad epochs dropped . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:30:51.989066 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ [&lt;Figure size 432x288 with 4 Axes&gt;] . s1[&#39;right_hand&#39;].plot_psd(fmin=0, fmax=40); . Using multitaper spectrum estimation with 7 DPSS windows . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:31:00.216750 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ bands = [(4, 8, &#39;Theta&#39;), (8, 12, &#39;Mu Rhythm&#39;), (12, 30, &#39;Beta&#39;)] . s1[&#39;right_hand&#39;].plot_psd_topomap(bands = bands, normalize = True); . Using multitaper spectrum estimation with 7 DPSS windows . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:31:05.375730 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ Get numpy array and visualize . print(&#39;EEG data is 3D numpy array (trials x channels x time samples) :&#39;, s1[&#39;right_hand&#39;].get_data().shape) . EEG data is 3D numpy array (trials x channels x time samples) : (144, 22, 321) . trial = 1 x = s1[&#39;right_hand&#39;].get_data()[trial,:,:] print(&#39;Channel x time samples :&#39;, x.shape) . Channel x time samples : (22, 321) . Plot single channel data from one trial . ch_names = s1.info[&#39;ch_names&#39;] ch_names . [&#39;Fz&#39;, &#39;FC3&#39;, &#39;FC1&#39;, &#39;FCz&#39;, &#39;FC2&#39;, &#39;FC4&#39;, &#39;C5&#39;, &#39;C3&#39;, &#39;C1&#39;, &#39;Cz&#39;, &#39;C2&#39;, &#39;C4&#39;, &#39;C6&#39;, &#39;CP3&#39;, &#39;CP1&#39;, &#39;CPz&#39;, &#39;CP2&#39;, &#39;CP4&#39;, &#39;P1&#39;, &#39;Pz&#39;, &#39;P2&#39;, &#39;POz&#39;] . # which channel to plot? channel = &#39;C4&#39; chIndex = [i for i, j in enumerate(ch_names) if j == channel] . plt.plot(x[chIndex[0], :]) plt.title(channel) plt.ylabel(&#39;Amplitude&#39;) plt.xlabel(&#39;Time samples&#39;) . Text(0.5, 0, &#39;Time samples&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-02-04T16:31:15.648784 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/",
            "url": "https://berdakh.github.io/blog/eeg/jupyter/2020/09/01/EEG-Visualization.html",
            "relUrl": "/eeg/jupyter/2020/09/01/EEG-Visualization.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Welcome Post!",
            "content": ". Nazarbayev University Brain-Machine Interfaces research focuses on the development and cross-validation of new neurotechnologies in Kazakhstan to improve the quality of life for disabled people, at the interface between engineering, robotics, and neuroscience. . Our research focus on developing neural interfaces to restore human motor functions after stroke, and developing brain-actuated assistive robotic systems for disabled persons as well as devising robust machine learning algorithms for Brain-Computer/Machine Interfaces. . We are always looking for researchers to join our team! Feel free to contact berdakho at gmail.com .",
            "url": "https://berdakh.github.io/blog/markdown/2020/01/14/Welcome.html",
            "relUrl": "/markdown/2020/01/14/Welcome.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "Publications",
          "content": "Journal Publications | Conference Publications | Patents | . Journal Publications . B. Abibullaev and A. Zollanvari. A Systematic Deep Learning Model Selection for P300-Based Brain-Computer Interfaces, IEEE Transactions on Systems, Man, and Cybernetics: Systems, DOI: 10.1109/TSMC.2021.3051136, 2021. [Link] . | B. Abibullaev, I. Dolzhikova and A. Zollanvari. A Brute-force CNN Model Selection for Accurate Classification of Sensorimotor Rhythms in BCIs, IEEE Access, DOI: 10.1109/ACCESS.2020.2997681, 2020. [Link] . | A. Zollanvari, M. Abdirash, A. Dadlani and B. Abibullaev. Asymptotically Bias-Corrected Regularized Linear Discriminant Analysis for Cost-Sensitive Binary Classification, IEEE Signal Processing Letters, 2019. [Link] . | B. Abibullaev and A. Zollanvari. Learning Discriminative Spatiospectral Features of ERPs for Accurate Brain-Computer Interfaces, IEEE Journal of Biomedical and Health Informatics, vol. 98, pp.1-12, 2019. [Link] . | B. Abibullaev, A. Zollanvari, B. Saduanov, and T. Alizadeh. Design and Optimization of a BCI-Driven Telepresence Robot Through Programming by Demonstration, IEEE Access, 2019, vol. 7. [Link] . | B Abibullaev, J An, SH Lee, JI Moon. Design and Evaluation of Action Observation and Motor Imagery based BCIs using NIRS, Measurement, vol. 98, pp. 250-261, 2017, Elsevier. [Link] . | N.A. Bhagat, A. Venkatakrishnan, B. Abibullaev, E.J. Artz, N. Yozbatiran, A. Blank, J. French, C. Karmonik, R.G.Grossman, M.K O’Malley, G. Francisco, J.L. Contreras-Vidal. Design and optimization of an EEG-based brain machine interface (BMI) to an upper-limb exoskeleton for stroke survivors, Front. Neurosci., vol. 10, March, 2016. [Link] . | J.G. Cruz-Garza, Z.R. Hernandez, T. Tse, E. Caducoy, B. Abibullaev, J.L. Contreras-Vidal. A novel experimental and analytical approach to the multimodal neural decoding of intent during social interaction in freely-behaving human infants, Journal of Visualized Experiments, doi:10.3791/53406, October, 2015. [Link] . | C.H. Park, J.H Seo, D. Kim, B. Abibullaev, H. Kwon, Y.H. Lee, M.Y. Kim, K. Kim, J.S. Kim, E.Y. Joo, S.B. Hong, (2015, Feb). Source Imaging in Partial Epilepsy in Comparison with Presurgical Evaluation and Magnetoencephalography, Journal of Clinical Neurology, 2015 Feb 17, 11:e12. [Link] . | B. Abibullaev, J An, S.H. Jin, and J.I. Moon. Classification of brain hemodynamic signals arising from visual action observation tasks for brain-computer interfaces: An fNIRS study, Measurement, 2014. Elsevier. [Link] . | B. Abibullaev, J An, S.H. Lee, S.H. Jin, and J.I. Moon. Minimizing inter-subject variability in FNIRS based brain computer interfaces via multiple-kernel support vector learning. Medical Engineering Physics, 2013. Elsevier. [Link] . | B. Abibullaev and J. An. Classification of frontal cortex hemodynamic response during cognitive tasks using wavelet transforms and machine learning algorithm, Medical Engineering Physics, 34(10):1394–410, 2012. Elsevier. [Link] . | B. Abibullaev and J. An. Decision support algorithm for diagnosis of ADHD disorder using electroencephalograms, Journal of Medical Systems, 36(4):2675–2688, 2011. Springer. [Link] . | B. Abibullaev, J. An, and J.I. Moon. Neural network classification of brain hemodynamic responses from four mental tasks, International Journal of Optomechatronics, 5(4):340–359, 2011. Taylor &amp; Francis. [Link] . | B. Abibullaev and H.D. Seo. A new QRS detection method using wavelets and artificial neural networks. Journal of Medical Systems, 35(4):683–691, 2011. Springer. [Link] . | B. Abibullaev, M.S. Kim, and H.D. Seo. Epileptic spike detection using continuous wavelet transforms and artificial neural networks, 8(1):33–48, 2010. International journal of wavelets, multiresolution and information processing, Worldscientific. [Link] . | B. Abibullaev, M.S. Kim, and H.D. Seo. Seizure detection in temporal lobe epileptic EEGs using the best basis wavelet functions. Journal of Medical Systems, 34(4):755–765, 2010. Springer. [Link] . | M.S. Kim, Y.C. Cho, B. Abibullaev, and H.D. Seo. Analysis of brain functions and classification of sleep stage EEG using daubechies wavelet. Sensors and Materials , 20(1):1–15, 2008. MYU, Japan. [Link] . | Conference Publications . A. Oleinikov, B. Abibullaev, M. Folgheraiter, “On the Classification of Electromyography Signals to Control a Four Degree-Of-Freedom Prosthetic Device,” in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC), July 20-24, Montreal, Canada. . | B. Saduanov, D. Tokmurzina, K. Kunanbayev and B. Abibullaev, “Design and Optimization of a Real-Time Asynchronous BCI Control Strategy for Robotic Manipulator Assistance”, in 2020 8th International Winter Conference on Brain-Computer Interface (BCI), 2020, GangWon, South Korea. . | A. Tuleuov and B. Abibullaev, “Deep Learning Models for Subject-Independent ERP-based Brain-Computer Interfaces,” in the 2019 9th International IEEE/EMBS Conference on Neural Engineering (NER), March 20-23, 2019, San Francisco, CA, USA. . | B. Abibullaev, Y. Orazayev, and A. Zollanvari, “Novel Spatiospectral Features of ERPs Enhances Brain-Computer Interfaces ,” in *Brain-Computer Interface (BCI), 2019 7th International Conference IEEE, 2019, GangWon, South Korea. . | B. Saduanov, D. Tokmurzina, T. Alizadeh, and B. Abibullaev, “Brain-computer interface humanoid pre-trained for interaction with people,” in 2018 ACM/IEEE International Conference on Human-Robot Interaction.1em plus 0.5em minus 0.4emACM, March 5-8, 2018, pp. 229–230, Chicago, IL, USA. . | A. Oleinikov, B. Abibullaev, A. Shintemirov, and M. Folgheraiter, “Feature extraction and real-time recognition of hand motion intentions from EMGs via artificial neural networks,” in Brain-Computer Interface (BCI), 2018 6th International Conference on.1em plus 0.5em minus 0.4em IEEE, 2018, pp. 1-5, GangWon, South Korea. . | G. Lee, S. H. Jin, S. T. Yang, J. An and B. Abibullaev, “Cross-correlation between HbO and HbR as an effective feature of motion artifact in fNIRS signal,” 2018 6th International Conference on Brain-Computer Interface (BCI), pp. 1-3, 2018, IEEE, GangWon, South Korea. . | B. Saduanov, T. Alizadeh, J. An, and B. Abibullaev, “Trained by demonstration humanoid robot controlled via a BCI system for telepresence,” in Brain-Computer Interface (BCI), 2018 6th International Conference on.1em plus 0.5em minus 0.4emIEEE, 2018, pp. 1–4, January, GangWon, South Korea. . | G. Lee, S. H. Jin, S. H. Lee, B. Abibullaev, and J. An, “fNIRS motion artifact correction for overground walking using entropy based unbalanced optode decision and wavelet regression neural network,” in Multisensor Fusion and Integration for Intelligent Systems (MFI), 2017 IEEE International Conference on.1em plus 0.5em minus 0.4emIEEE, 2017, pp. 186–193, Daegu, South Korea. . | A. Zhumadilova, D. Tokmurzina, A. Kuderbekov and and B. Abibullaev. Design and Evaluation of a P300 Visual Brain-Computer Interface Speller in Cyrillic Characters. In the 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), IEEE, 2017, Aug 28 - Sept 1, Lisbon, Portugal. . | B. Abibullaev.Learning Suite of Kernel Feature Spaces Enhances SMR-Based EEG-BCI Classification. In the 5th International Winter Conference on Brain-Computer Interface, IEEE, 2017, January 9-11, GangWon, South Korea. . | D. Nurseitov, A. Serekov, A. Shintemirov and B. Abibullaev. Design and Evaluation of a P300-ERP based BCI System for Real-Time Control of a Mobile Robot. In the 5th International Winter Conference on Brain-Computer Interface, IEEE, 2017, January 9-11, Gangwon , South Korea. . | B. Abibullaev and J An. On Robust Classification of Hemodynamic Signals for BCIs via Multiple Kernel ν-SVMs. Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference, 2016, Oct10-15, Daejeon, South Korea. . | J.G. Cruz-Garza, Z.R. Hernandez, M. Megjhani, B. Abibullaev, T. Tse, E. Caducoy, and JL Contreras-Vidal.Neural development of social cognition in the first two ears of life - Early findings. In Society for Neuroscience 2015 Oct 17-21; Chicago, USA. . | Z.R. Hernandez, A.J. Arenas-Castellanos, J.G. Cruz-Garza, M. Megjhani, B. Abibullaev, Sri. R.P. Maddi, T. Tse, C. Armstrong, W. Long, J.L. Contreras-Vidal, Decoding Intent From Non-invasive EEG in Freely Behaving Infants In Ninth Biennial Meeting of the Cognitive Development Society 2015 Oct 9-10; Ohio, USA. . | A.J. Arenas-Castellanos, Z.R. Hernandez, J.G. Cruz-Garza, M. Megjhani, B. Abibullaev, Sri. R.P. Maddi, T. Tse, C. Armstrong, W. Long, J.L. Contreras-Vidal, A developmental Analysis of Behaviors Related to the Mirror Neuron System in 6-24 Months Infants, In Ninth Biennial Meeting of the Cognitive Development Society 2015 Oct 9-10; Ohio, USA. . | Z.R. Hernandez, J.G. Cruz-Garza, T. Tse, E. Caducoy, B. Abibullaev, J.L. Contreras-Vidal. Supervised Classification of Intended Behaviors Using Electroencephalography (EEG) from Freely-Behaving Infants: Early Findings. In 12th Annual GCC Conference on Theoretical and Computational Neuroscience, Rice University, Houston, TX, United States, February 6-7, 2015. . | N. A. Bhagat, A Venkatakrishnan, B. Abibullaev, E. J. Artz, A.A. Blank, J. A. French, N. Yozbatiran, and R. G. Grossman, M. K. OMalley, J. L. Contreras-Vidal, G. E. Francisco, Control of a Therapeutic Exoskeleton to Facilitate Personalized Robotic Rehabilitation of the Upper Limb. , Westin Arlington, Nov. 19-20 2014, United States. . | C.H.Park, B. Abibullaev, E.Y. Joo, S.C. Hong, and S.B. Hong. The evaluation of accuracy and clinical usefulness of 3D EEG source localization analysis. In the 19th Korean Epilepsy Congress, Seoul, Republic of Korea, June 12-14 2014. . | S.H. Lee, J. An, G. Jang, S.H. Jin, B. Abibullaev, and J.I. Moon. Neural activity during observation, imagery, and execution of eating: An fNIRS pilot study. In the 19th Annual Meeting of the Organization for Human Brain Mapping, Seattle, WA, United States, June 16-20 2013. . | J. An, S.H. Jin, S.H. Lee, G. Jang, B. Abibullaev, J. Ahn, H. Lee, and J.I. Moon. Cortical activation pattern for grasping during observation, imagery, execution, FES, and observation-FES integrated BCI: An fNIRS pilot study. In the 35th Annual Int. Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Osaka, Japan, July 3-7 2013. . | J. An, S.H. Lee, S.H. Jin, B. Abibullaev, G. Jang, J. Ahn, H. Lee, and J.I. Moon. The beginning of neurohaptics: Controlling cognitive interaction via brain haptic interface. In the 2013 IEEE International Winter Workshop on Brain-Computer Interface, Gangwon Province, Korea, Feb. 18-20 2013. . | B. Abibullaev, J. An, J.I. Moon, S.H. Lee, and S.H. Jin. A study on the BCI-Robot assisted stroke rehabilitation framework using brain hemodynamic signals. In the 9th Int Conference on Ubiquitous Robots and Ambient Intelligence, IEEE proceedings, Daejon, Korea, November 26-29,2012. . | B. Abibullaev, J. An, S.H.Lee, S.H. Jin, J.I Moon, H.J. Lee. Decoding of Brain Hemodynamic Responses for Brain-Computer Interfaces via Ensemble Support Vector Learning. In the Human-Computer Interaction Conference, Gangwon-Do, Korea, January 11-13,2012. . | B. Abibullaev, W.S. Kang, S.H. Lee, J. An, and H.D. Seo. Near-infrared spectroscopy in the analysis of functional brain activity during cognitive tasks. In 2010 IEEE Sensors Conference, Hawaii, United States, November 1-4, 2010. . | B. Abibullaev, W.S. Kang, S.H. Lee, and J. An. Recognition of brain hemodynamic mental response for brain-computer interface. In International Conference on Control Automation and Systems, IEEE proceedings, Gyeonggi-do, Korea, October 27-30 2010. . | S.H. Lee, B. Abibullaev, W.S. Kang, and J. An. Analysis of attention deficit hyperactivity disorder in EEG using wavelet transform and self organizing maps. In International Conference on Control Automation and Systems, IEEE proceedings, Gyeonggi-do, Korea, October 27-30 2010. . | W.S. Kang, B. Abibullaev, S.H. Lee, and J. An. Path planning algorithm using the values clustered by K-means. In the 15th Int. Symposium on Artificial Life and Robotics, Japan, February 4-6 2010. . | B. Abibullaev, H.D. Seo, W.S. Kang, and J. An. A wavelet-based method for detecting and localizing epileptic neural spikes in EEG signals. In the 2nd Int. Conf. on Interaction Sciences: Information Technology, Culture and Human. ACM, Seoul, Korea, Nov. 24- 26 2009. . | B. Abibullaev and H.D. Seo. Epileptic seizures detection using continuous time wavelet based neural networks. In the 6th International Conference on Information Technology: New Generations, IEEE Computer Society, Las Vegas,Nevada, United States, April 27-29 2009. . | W.S. Kang, B. Abibullaev, S.H. Lee, and J. An. A study on brain activation during playing a computer game using fNIRS. In the 32th conference of Korean Info. Proc. Soc., Seoul Korea, November 22-24,2009. . | B. Abibullaev, H.D. Seo, and M.S. Kim.Classification system of EEG during cognitive mental tasks. In Int. Conference on Engineering and ICT, Melaka, Malaysia, November 27-29, 2007. . | H.D. Seo and B. Abibullaev. Analysis of EEG signals by the continuous wavelet transforms. In the 5th Int. Joint Conference on Global Academic Networking, Vladivastok, Russia, June 7-9 2007. . | Patents . J. An, S.H. Jin, S.H. Lee, J.I. Moon, B. Abibullaev, J.H. Ahn and G.H. Jang. Rehabilitation Training System and Method. : 9,081,890, Washington, DC: United States. Patent and Trademark Office, 2015 » Link «. . | J. An, S.H. Jin, S.H. Lee, J.I. Moon, B. Abibullaev, J.H. Ahn and G.H. Jang. Self-directed Rehabilitation Training Method Combining Brain Signals and Functional Electrostimulation. : 17077057, Application number: 14049302, 09-OCT-2013, United States, » Link «. . | J. An, S.H. Jin, S.H. Lee, B. Abibullaev, J.I. Moon , J.H. Ahn and G.H. Jang. Combining Brain Signals and Functional Electrostimulation Self-Directed Rehabilitation Method, Patent Registration Number: 101501524000, 05.03.2015. South Korea, » Link «. . |",
          "url": "https://berdakh.github.io/blog/publication/",
          "relUrl": "/publication/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://berdakh.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}